# Đặc điểm của neutron truyền thống
0. Hình dạng input, output của 1 neutron thông thường
    ![](images/0.%20neutron.png)
1.  Công thức tính toán output của 1 neutron. Và các tham số w0 -> wm là thứ máy tính sẽ phải xác định. Gọi là ma trận trọng số. Là các tham số trainable. Có thể thấy số lượng các tham số này phụ thuộc vào độ dài của feature map.
    ![](images/0.%20formular.png)

# Nhược điểm mạng neutron truyền thống trong train ảnh.
1. Không tận dụng thông tin không gian
    - Ảnh được biểu diễn dưới mạng tensor, trong khi neutron truyền thống chỉ xử lý dữ liệu dưới dạng vector 1 chiều. Do đó cần chuyển đổi ảnh về vector 1 chiều trước khi đưa vào neutron.
    - Quá trình này làm mất đi cấu trúc không gian của hình ảnh (như mối quan hệ giữa các pixel gần nhau).
2. Số lượng các tham số cần train quá lớn
    - 1 mạng neutron có thể có rất nhiều neutron, đặc biệt là các mạng deep learning còn nhiều nữa.
    - Ảnh là input của 1 neutron, mà ảnh thường có kích thước (112*112) pixel, ... Khi đổi nó thành feature map 1 chiều thì độ dài của nó sẽ rất lớn. Do đó các tham số cần train đối với 1 neutron khi input là ảnh là rất lớn.
    - Với 1 lượng tham số cần train khổng lồ như vậy đòi hỏi thời gian rất lớn, hạ tầng mạnh mẽ mới có thể thực hiện được. Chưa kể vấn đề mạng chỉ thực sự tốt khi được train với big data, ảnh và video có chất lượng cao.

# CNN
0. Các phép tình convolutionvà các khái niệm convolution/pading/strike: https://nttuan8.com/bai-5-gioi-thieu-ve-xu-ly-anh/

1. Thay vì train 1 chuỗi các trọng số w trong mỗi neutron. Ta sẽ thực hiện train các trọng số của 1 kernel có kích thước xác định (gọi là filter). Ví dụ với ảnh 2D:
        ![](images/0.%202D-kernel.png)
    Có thể thấy dù input ảnh là bao nhiêu thì số lượng tham số phải train chỉ là 9 không đổi (do kernel có kích thước 3*3) theo mặc định.
    Với ảnh RGB hay các ảnh có color channel =3 thì filter sẽ là 1 ma trận 3 chiều.
        ![](images/0.%203D-kernel.png)
    Có thể thấy neutron này vẫn cần phải train bias để tăng tính tổng quát hóa và activation funciton.
    Ngoài ra có thể  thấy tensor ảnh không cần phải chuyển đổi gì về feature 1 chiều, từ đó giữ được đặc điểm lân cận của các pixel.

# Các khái niệm CNN
0. Khái niệm
    Kích thước không gian: ám chỉ 2 chiều đầu của tensor là height và width.

1. CNN block
    - 1 Convolution layer (chứa các neutron CNN như ở trên với activation function thường là Relu để tăng tốc độ train và hiệu suất mô hình). Mỗi neutron là 1 **kernel/filter** => 1 convolution layer có thể chứa nhiều filter. Ví dụ: CNN có 6 filter với kích thước (3 * 3 * 3). Conv dùng để học các đặc trưng lân cận của image (để ý cách tính di chuyển của CNN)
    - 1 Pooling layer. Dùng để giảm kích thước không gian thu được của feature map thu được sau Convolution layer. Thường là max polling hoặc average pooling.
    - Mạng CNN deep learning thông thường là các CNN block được lặp lại nhiều lần và có thể thêm vào 1 số layer như: 
        - Batch Normalization: Giúp ổn định quá trình huấn luyện, tăng tốc độ hội tụ.
        - Dropout: Giảm thiểu hiện tượng overfitting bằng cách ngẫu nhiên loại bỏ một số neuron trong quá trình huấn luyện.
        - Fully Connected Layer (FC Layer): Ở cuối mô hình, các đặc trưng sẽ được làm phẳng (flatten) và đưa vào các lớp FC để thực hiện phân loại.
        - Skip connection: trong retnet, ...
        - ....

2. 1 * 1 Conv, 3 * 3 Conv, ... trong kiến trúc các mạng neutron hiện đại. Gọi là lớp tích chập 1 * 1, ...
    Với 1 * 1 Conv:
        **Mục đích: Giảm số chiều cuối cùng của tensor và giữ nguyên các chiều còn lại (thường là chiều rộng và cao, chiều cuối cùng là bao nhiêu đó.)**
        **Nói là 1 * 1 nhưng bản chất là 1 khối 3 chiều 1 * 1 * Cin (với Cin là số lượng kênh đầu vào)**
        
        Ví dụ: 1 feature map đầu vào là 4 * 4 * 8. 
            - Với 4 * 4 là chiều cao và chiều rộng (kích thước của không gian)
            - Với 8 là số lượng kênh (feature được chính xuất từ convolution layer trước đó, đọc tiếp sẽ hiểu tại sao nó lại lớn vậy trong khi RGB chỉ có 3)
            Mục tiêu giảm số lượng chiều sau cùng xuống 4. Thiết kế 1 Convolution gồm 4 filter 1 * 1. 
            - Do kernel có kích thước 1 * 1 nên về chiều rộng và cao của output sẽ giống hệt ban đầu. Output của mỗi filter là 1 ma trận (4,4)
            - Kết quả của 4 filter sẽ được gộp lại tạo thành 1 feature mới => 4 cái ma trận 2 chiều (4,4) gộp lại thành feature (4,4,4)
    Với 3 * 3 Conv: **Lớp tích chập phổ biến nhất trong mạng CNN tiên tiến.**
        Mục đích: **Học các đặc trưng của pixel trong 1 vùng nhỏ hơn của hình ảnh nhưng vẫn đủ lớn để nắm bắt được các feature quan trọng như cạnh và góc.**
        Kết quả: 
            - Làm giảm kích thước không gian (height, width) 1 chút, đặc biệt là khi thiết kế stride lớn. **Thực nghiệm cho thấy các filter 3 * 3 có thể trích xuất các đặc trưng mạnh mẽ**
            - Hỗ trợ xây mạng neutron sâu hơn với ít tham số hơn. **1 filter 5 * 5 có thể được thay bằng 2 lớp filter 3 * 3 với tham số cấn train ít hơn (25 > 2 * 9)**
    Với 5 * 5 Conv và các conv cao hơn:
        Mục đích: **Học các đặc trưng tổng quát hơn của vật thể**. 
        Filter 3 * 3 có nhược điểm, nó chỉ là 1 kernel học (9 * Cin) pixel cùng lúc, khó phát hiện các feature của vật thể có kích thước lớn hơn 9. Trong nhiều trường hợp hay bằng mắt thường, ta có thể thấy vật thể có những nét đặc trưng rất đặc biệt và nằm trong 1 vùng ảnh rất lớn. 
        Với các filter lớn, mô hình sẽ học được các đặc trưng lớn đó tốt hơn so với filter 3 * 3.
        Hạn chế: **Không tốt cho việc phát hiện các feature của vật thể có kích thước rất nhỏ và các tham số cần train lớn.**

3. Fully connected layer:
    Sau khi trải qua nhiều Convoluton Block thì model đã học được kha khá các đặc điểm của ảnh. Tuy nhiên đầu ra của nó thường vẫn là tensor nhiều chiều. Lúc này sẽ trải qua 1 lớp gọi là Faltten để đưa tensor này về vector 1 chiều rồi đưa vào Fully connected layer này để dự đoán ra embedding.

4. Pooling layer:
    Thường sau mỗi CNN layer sẽ là 1 Pooling layer, đặc biệt là trong các mạng CNN cơ bản và truyền thống. Kernel của các lớp Pooling thường được đặt là 2. **Lợi ích các lớp pooling chung là giảm kích thước không gian của kernel theo cấp số nhân, kernel là 2 thì giảm 2 lần**:
    - Giảm kích thước không gian (cao và rộng) của feature map mà không thay đổi số lượng kênh (ngược với 1 *1 Conv)=> Giảm tham số train trong những layer tiếp theo.
    - Nó **không có tham số gì cần train cả**. Hiểu về MaxPooling và AveragePooling tại: https://nttuan8.com/bai-6-convolutional-neural-network/
    - MaxPolling: Áp dụng 1 kernel với kích thước nào đó và output là giá trị pixel lớn nhất trong vùng kernel đó. Do lấy giá trị lớn nhất nên ngoài giảm kích thước, l**ớp này có khả năng trích xuất những đặc trưng nổi bật nhất của vật thể dù ảnh có xê dịch nhó (tùy vào size của kernel ta config)**
    - AveragePolling: Giống với MaxPolling, nhưng thay vì lấy giá trị lớn nhất mỗi khi áp dụng kernel thì nó sẽ lấy giá trị trung bình. **Lớp Pooling này thường ít được sử dụng hơn MaxPolling do có thể làm mất những thông tin quan trọng**
    - Global Pooling (Global Average Pooling - GAP, Global Max Pooling - GMP): Là 1 biến thể đặc biệt của Pooling, được sử dụng phổ biến trong các kiến trúc hiện đại như ResNet, MobileNet, EfficentNet.
        - Global Average Pooling (GAP): Thay vì áp dụng pooling cho từng patch nhỏ, GAP sẽ tính trung bình cho toàn bộ mỗi kênh của feature map.
            Ví dụ: Nếu đầu vào là một feature map kích thước 7×7×512, GAP sẽ tính trung bình cho mỗi kênh 7×7, và kết quả sẽ là một vector 1×1×512.
            **Tác dụng: không được dùng làm pooling layer sau mỗi conv layer mà được sử dụng để thay thế Fully Connected layer cuối cùng của mạng neutron và Flatten.**
            Ưu điểm: Tăng khả năng tổng quát hóa và giảm nguy cơ overfitting. Bất biến với kích thước không gian, nghĩa là có thể áp dụng cho các hình ảnh có kích thước không gian khác nhau, embedding sau cùng sẽ có len không đổi.
            **Lớp FC có rất nhiều tham số, dễ gây ra overfitting khi kích thước dữ liệu đầu vào lớn. GAP giúp giảm số lượng tham số, tăng khả năng tổng quát hóa và giúp mạng có thể xử lý các hình ảnh có kích thước khác nhau.**
        - Global Max Pooling (GMP): Thay vì lấy giá trị trung bình, GMP sẽ lấy giá trị lớn nhất cho mỗi kênh của feature map.
            Ví dụ: Với đầu vào là 7×7×512, GMP sẽ lấy giá trị lớn nhất của mỗi kênh 7×7, kết quả là 1×1×512.
            **Tác dụng: Chọn lọc đặc trưng mạnh nhất của mỗi kênh. Thường được sử dụng trong các kiến trúc cần đặc trưng nổi bật hơn.**

5. Batch Normalization (BN)
    Là 1 kỹ thuật rất phổ biến trong các mạng deep learning giúp tăng tốc độ huấn luyện mô hình.
    Giải quyết 1 phần các vấn đề như:
        **Internal Covariate Shift**: Trong quá trình huấn luyện, phân phối của đầu vào mỗi layer có thể thay đổi khi trọng số của các layer trước đó thay đổi. Điều này khiến mạng khó huấn luyện hơn.
        **Gradient Exploding/Vanishing**: Khi đầu vào quá lớn hoặc quá nhỏ, gradient có thể bị "bùng nổ" hoặc "tiêu biến", làm cho quá trình tối ưu hóa trở nên khó khăn.
        **Batch Normalization** giúp ổn định và giảm bớt những vấn đề trên bằng cách chuẩn hóa đầu vào của mỗi layer, giúp gradient ổn định hơn và tăng tốc độ hội tụ.
    **BN chuẩn hóa đầu vào dựa trên mean và variance của mini-batch**, sau đó áp dụng một phép biến đổi tuyến tính để đảm bảo khả năng biểu diễn của mạng.
        ![](images/0.%20batch%20nornamlize.png)
    Trong CNN, BN thường được chèn sau Convolution layer và trước Activation layer (ReLU, Sigmoid).
    Trong các mạng nơ-ron thông thường (Fully Connected Network), BN thường được chèn sau mỗi lớp Fully Connected.
    Nhược điểm:
        Phụ thuộc vào batch size: BN hoạt động tốt nhất khi batch size không quá nhỏ. Với batch size nhỏ, mean và variance có thể không chính xác.
        Phức tạp hơn khi sử dụng Recurrent Neural Networks (RNNs): BN khó áp dụng trực tiếp trong các mạng RNN do sự thay đổi qua từng thời điểm (time step).
        Không hiệu quả khi inference: Khi thực hiện inference (dự đoán), không có mini-batch để tính mean và variance, phải sử dụng mean và variance đã được ước lượng trong quá trình huấn luyện.