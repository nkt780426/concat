https://github.com/iduta/iresnet

# Abstract

Cải thiện Resnet ở 3 điểm:
- Luồng thông tin qua các lớp mạng: Cải tiến cách thức thông tin di chuyển qua các lớp của mạng.
- Khối dư thừa (Residual Block): Cải tiến cách thức các khối dư thừa được sử dụng trong mạng để giảm thiểu sự mất mát thông tin khi đi qua các lớp sâu.
- Đường tắt chiếu (Projection Shortcut): Cải tiến cách thức kết nối các lớp, cho phép thông tin được chuyển qua các lớp mà không bị thay đổi quá nhiều.
**Không làm tăng độ phức tạp của mô hình so với resnet truyền thống**
Cải thiện độ chính xác và sự hội tụ:
Đào tạo mạng sâu: thiết lập 1 cột mốc mới về độ sâu của CNN với 404 lớp trên ImageNet và 3002 trên CIFAR-10 và CIFAR-100

# Introduce

1. Ý tưởng cốt lõi của resnet.
    - Được sử dụng làm backbone của rất nhiều tác vụ phúc tạp.
    - Ý tưởng cốt lõi là tạo điều kiện cho việc học các ánh xạ đơn vị (identity mappings) hay Redisual Block bằng cách sử dụng shortcut/skip connection để thêm input vào output của redisual block.
    - Ý tưởng này góp phần giải quyết **degradation problem** nhưng không triệt để. Cụ thể mạng tăng từ 152 đến 200 trên imagenet, kết quả thu được trở nên kém hơn. Điều này cho thấy vấn đề tối ưu hóa nghiêm trọng.

2. Đề xuất cải tiến kiến trúc giúp truyền thông tin qua mạng hiệu quả hơn.
    - Tách mạng thành các stage và áp dụng 1 khối redisual block khác nhau tùy thuộc vào từng stage. => đề xuất của chúng tôi có thể học các mạng cực kỳ sâu, mà không gặp khó khăn về tối ưu hóa khi độ sâu tăng lên.
    - Các đường tắt chiếu (projection shortcuts):
        - Trong Resnet, khi dimentions của 1 building block không khớp với block tiếp theo thì project shortcut phải được sử dụng. Công trình [6] chỉ ra rằng project shortcut không góp phần cho vấn đề suy giảm đạo hàm nhưng lại đóng vai trò quan trọng vì nằm trên đường truyền thông tin chính và có thể làm xáo trộn tín hiệu hoặc gây mất thông tin.
        - Giới thiệu project shortcuts mới không có tham số mang lại hiệu suất đáng kể hơn.

    - BottleneckBlock được sử dụng trong Resnet được sử dụng để kiểm soát số lượng tham số và chi phí tính toán khi độ sâu tăng đáng kể. Tuy nhiên, trong thiết kế này, tích chập chịu trách nhiệm học không gian (spatial convolution) nhận ít channel cho cả input và output.
        - Chúng tôi đề xuất một building block mới tập trung vào spatial convolution. Kiến trúc của chúng tôi có số lượng kênh không gian nhiều gấp bốn lần so với ResNet gốc, nhưng vẫn giữ số tham số và chi phí tính toán trong tầm kiểm soát.

# Relatework

- Resnet rất hiệu quả trong việc huấn luyện visual recognition system.

- Công trình [7] giới thiệu ResNets **pre-activation ResNets** bằng cách đề xuất 1 thứ tự mới cho các thành phần trong building block để cải thiện signal propagation. => Iresnet cũng đề cập đến flow information trong network nhưng mà đề xuất mỗi stage 1 building block riêng. Kết quả thu được tốt hơn khi mạng sâu cho đến rất sâu so với [7]

- Công trình [16] sử dụng **group convolution** để chia nhỏ việc tính toán conv trên 2 GPU nhằm vượt qua giới hạn về tài nguyên tính toán. [35] cũng sử dụng nhưng chú trọng hơn về việc cải thiện hiệu suất của resnet. => Iresnet cũng sử dụng group convolution nhưng khác ở architect. Giới thiệu building block architech có nhiều hơn gấp 1 lần filters spatial. Hiển nhiên kết quả accuracy cải thiện.

- [10] và [33] sử dụng thêm các khôi **squeeze-and-excitation** và **non-local blocks** để cải thiện hiệu suất. Nhưng nó khôn giải quyết được vấn đề suy giảm đạo hàm mà còn làm mạng phức tạp hơn.

# 3. Improved residual networks (Iresnet)

## 3.1. Improved information flow through the network (Cải thiện luồng thôn tin qua mạng)

![](images/7.%20Resnet%20+%20Iresnet.png)

- Resnet được tạo thành bởi nhiều Residual Block xếp chồng lên nhau. Kiến trúc có thể thấy như hình 1a. Công thức của 1 residual block như sau

![](images/7.%20Residual%20Block%20Formular.png)

- Nhược điểm của công thức và kiến trúc resnet là có 1 hàm Relu nằm ở probation path. Nó có thể làm mất tín hiệu truyền ngược trong quá trình huấn luyện.

- [7] đề xuất pre-activation block bằng cách di chuyển lớp BN và Relu cuối cùng lên đầu tiên như hình 1b. Tuy nhiên cả Resnet và pre-activation Residual block đều không tối ưu và tồn tại những vấn đề khác nhau.
    - Không có BN trong ở đường truyền fullu connected.gây khó khăn trong tính toán khi số lượng block tăng lên (cả Resnet cũng gặp phải)
    - Có 4 projections shortcuts (4 lớp 1x1 trong main path). Với pre-activation ResNet có 4 state và kết thúc bằng Con 1x1 mà không có chuẩn hóa đầu ra => Iresnet sử dụng BN lúc kết thúc quá trình học mỗi state.

**Đề xuất**
- Mạng được chia ra làm các state khác nhau, mỗi state ResBlock có kiến trúc khác nhau như hình 1c. Sử phân tách các state được xác định dựa theo spatial size và số lượng output channels. Khi số output spatial size hay output channels thay đổi thì nó sẽ nhảy sang 1 state khác.
- Ví dụ Resnet 50 sẽ có 4 state. State 1 có 3 block, state 2 có 4, state 3 có 6 và state 4 có 3. Trong mỗi state sẽ có 3 phần Start ResBlock, các mid ResBlocks và 1 End RedBlocks(Mỗi state sẽ có 1 end ResBlock khác nhau)
- Mid ResBlock đầu tiên của mỗi state, BN ở đầu được bỏ đi vì đã được chuẩn hóa bởi cuối Start Block trước đó.
- Kiến trúc này không làm tăng độ phức tạp của mạng so với Resnet truyền thống (cùng số lương parametes phải train) và chỉ thay đổi cách sắp xếp kiến trúc.

**Điểm hơn**
- So với Resnet gốc[6], sẽ chỉ có 4 Relu activation function trên main path của mạng bất kể độ sâu của mạng ra sao. Còn Resnet số Relu sẽ tỷ lệ thuận với độ sâu của mạng. Điều này tránh hiện tượng suy giảm tín hiệu khi thông tin được truyền qua nhiều lớp.
- So với [7], End ResBlock của mỗi giai đoạn được hoàn thiện với BN và ReLU, được xem như sự chuẩn bị cho giai đoạn tiếp theo, giúp ổn định và chuẩn bị tín hiệu để bước vào giai đoạn mới. Trong Start ResBlock của chúng tôi, có một lớp BN sau lớp convolution cuối cùng, nhằm chuẩn hóa tín hiệu, chuẩn bị cho việc thực hiện phép cộng từng phần tử với projection shortcut (cũng cung cấp tín hiệu đã được chuẩn hóa). => Tạo điều kiện tốt hơn cho việc học bằng cách cung cấp 1 đường dẫn truyền thông tin tốt hơn. **Mạng có thể linh hoạt chọn ResBlock nào cần sử dụng và ResBlock nào cần loại bỏ (Bằng cách đặt trọng số 0 trong quá trình học)**.
- Hiệu suất mạng tốt hơn chứng tỏ cách tiếp cận là tốt hơn :v.

## 3.2. Improved projection shortcut

![](images/7.%20Residual%20Block%20Formular.png)

- Trong kiến trúc gốc, khi kích thước gốc x không khớp với kích thước đầu ra của F, một projection shortcut được áp dụng lên x (thay vì dùng một identity shortcut, xem phương trình 1). Minh họa hình 2a

![](images/7.%20project%20shortcut.png)

- Shortcut chiều gốc sử dụng 1 Conv 1x1 để chiếu các channels của x thành số kênh đầu ra F ( stride của tích chập 1×1 là 2, giúp điều chỉnh kích thước không gian giữa x và đầu ra của F). Sau đó 1 lớp BN được áp dụng trước khi thực hiện phép cộng phần tử với đầu ra của F. => Điều chỉnh cả số channel và không gian bằng Conv 1x1.

- Điều này gây tổn thất thông tin đáng kể vì nó bỏ qua 75% activation của feature map khi giảm kích thước không gian xuống còn 1 nửa và không có tiêu chí nào loại 25% này => Thành nhiễu ảnh hưởng tiêu cực đến thông tin đầu vào của Residual Block sau.

- Giới thiệu 1 project shortcut ở hình 2b. Họ tách biệt ra làm 2 thành phần spatial projection (chiếu không gian) và channel projection (chiếu kênh).
    - Đối với spatial projection, thực hiện phép max pooling 3×3 với stride là 2
    - Đối với channel projection, thực hiện Conv 1x1 với stride là 1
    - Kết quả sau cùng đi qua BN

- Shortcut được đề xuất giúp giảm tổn thất thông tin trong main path.
Ngoài mục tiêu chính giảm loss information và perturbation of the signal (nhiễu tín hiệu), còn có 2 lý do nữa mà phương pháp projection shortcut này được đề xuất.
    - Cải thiện tính bất biến theo dịch chuyển (translation invariance): Việc sử dụng max pooling trong Start ResBlock của mỗi giai đoạn chính giúp tăng cường khả năng bất biến theo dịch chuyển, từ đó cải thiện hiệu suất nhận dạng tổng thể.
    - Kết hợp giữa "downsampling mềm" và "downsampling cứng":
        - Start ResBlock trong mỗi giai đoạn, thực hiện downsampling, có thể được coi là sự kết hợp giữa:
            - Downsampling mềm (weighted downsampling), được thực hiện bởi tích chập 3×3 (với trọng số học được).
            - Downsampling cứng (hard downsampling), được thực hiện bởi max pooling 3×3
        - Tương tự, chúng ta có thể so sánh với soft assignment (như trong mã hóa Fisher Vectors) và hard assignment (như trong mã hóa VLAD). Mỗi loại downsampling mang lại lợi ích bổ sung:
            - Downsampling cứng hữu ích cho phân loại (chọn phần tử có kích hoạt cao nhất).
            - Downsampling mềm giúp không mất hoàn toàn ngữ cảnh không gian (hỗ trợ định vị tốt hơn, nhờ quá trình chuyển đổi mượt mà hơn giữa các phần tử).

- Chi phí tính toán tăng thêm dành cho shortcut là không đáng kể vì chiir thêm max pooling.

## 3.3. Grouped building block (khối xây dựng nhóm)

- Bottleneck building block được giới thiệu trong Resnet gốc với mục đích thực tiễn nhằm duy trì chi phí tính toán hợp lý khi tăng độ sâu của mạng. Thiết kế này bao gồm:
    - Conv 1x1: giảm số lượng channel
    - Bottleneck Conv 3x3: triển khai số lượng input/output channel nhỏ nhất.
    - Conv 1x1: tăng số lượng channels lại như cũ
- Mục tiêu giảm chi phí tính toán và số lượng tham số bằng cách thực hiện tích chập 3×3 trên số lượng kênh nhỏ hơn. Tuy nhiên, Conv 3×3 rất quan trọng vì nó là thành phần duy nhất có khả năng học spatial patterns, nhưng trong thiết kế bottleneck, số lượng kênh vào/ra của nó lại bị giảm.

**Đề xuất**
- Cải thiện building block trên bằng cách đưa input/output channels vào Conv 3x3 này. Thiết kế này được gọi là **ResGroup**. Nó từng được [16] sử dụng dể phân phối mô hình trên 2 GPU để khắc phục hạn chế chi phí tính toán và bộ nhớ. Gần đây [35] đã khai thác tích chập nhóm để cải thiện độ chính xác.
    - Với Convlution chuẩn, mỗi output channel sẽ kết nối với tất cả input channel. **Ý tưởng chính của group convolution là chia input thành các group và thực hiện tích chập độc lập trên từng group** => Giảm parameter và FLOPS
    - Công thức tính số lượng Param cần train và FLOP như sau (không quan tâm).
    ![](images/7.%20Group%20Convolution.png)

- Kiến trúc được đề xuất có chi phí tính toán và số lượng tham số tương tự ResNet-50, được chứng minh trong bảng sau
![](images/7.%20Resgroup%20+%20ResGroupFix.png)
- Trong kiến trúc trên Group Convolution được sử dụng với kernet 3x3. Mỗi lớp tích chập đi kèm với BN và Relu, được minh họa bởi hình sau
![](images/7.%20Group%20Convolution.png)

- Trong phần experiment họ sẽ show accuracy so với resnet thuần hay baseline và ResNext[35]

# 4. Experment

1. Cấu hình thử nghiệm:
    ![](images/7.%20Config.png)
2. Results of iResNet on ImageNet
    - iResnet = ResStages + projection shortcut
    - Table 2: so sánh tỷ lệ lỗi giữa iResnet và Resnet[6] và pre-activation Resnet với 50, 101, 152 và 200 layer
    ![](images/7.%20Table%202.png)
    - Kết quả:
        - iResnet vượt trội hơn ở mọi thông số.
        - Tiếp tục tăng số lượng depth lên thì training vẫn đang hội tụ và tác giả gặp vấn đề về tài nguyên
        ![](images/7.%20Table%203.png)