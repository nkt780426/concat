# Nguồn
https://arxiv.org/pdf/1512.03385
# Giới thiệu
Được giới thiệu vào 2015 bởi nhòm Microsoft research và tạo ra bước đột phá lớn trong lĩnh vực nhận diện ảnh. Giành được top 1 trong cuộc thi Image Net trong lĩnh cực Image Recognition. Mấu chốt của nó là đã giải quyết được vấn đề gradient vanishing khi huấn luyện mạng rất sâu.
Gradient vanishing/expliding
- Ở các mô hình CNN trước đó, khi mạng neutron càng sâu thì hiệu suất có khi cong giảm đi và không học được gì. 
- Nguyên nhân là thuật toán backprobagation lan truyền ngược lại tín hiệu lỗi. Trong quá trình lan truyền từ layer cuối cùng về layer đầu, delta w liên tục được tính đạo hàm. Do đó với mạng neutron càng sâu thì delta w mà các layer đầu tiên nhận được sau cùng sẽ càng nhỏ và gần như bằng 0 => Trọng số của các layer đầu tiên gần như không thay đổi và gần như không thể học.
- Resnet giải quyết vấn đề trên bằng Residual Block. Input của 1 lớp sẽ được cộng trực tiêp vào đầu ra của lớp đó qua 1 đường tắt (skip connection/short cut).
# Residual Block
0. Giả sử x là đầu vào của residual block, và F(x) là hàm học được qua một hoặc nhiều lớp tích chập (Convolution) hay là các layer bên trong block. Output của Redisual Block sẽ là Output=F(x)+x.

        ![](images/1.%20residual%20block.png)

    Dễ thấy có 1 skip connect, đường này giúp các gradient dễ dàng đi qua các residual block mà không bị tiêu biến.

1. Kiến trúc 1 Residual block.
    Đầu vào là một tensor 𝑥 với shape (H,W,C).
    Lớp Conv: 𝑥 →Conv1→ReLU→Conv2x.
    Kết quả là: F(x).
    Đường tắt (skip connection): F(x)+x.
    Áp dụng ReLU sau khi cộng.

# Kiến trúc phổ biến
ResNet-18, ResNet-34, ResNet-50, ResNet-101, ResNet-152 là các phiên bản phổ biến, với số lượng lớp lần lượt là 18, 34, 50, 101, và 152.
Trong các kiến trúc này, các khối Residual Block được sắp xếp xen kẽ với các lớp tích chập và pooling.
