# 4. Algorithms

Pháº§n nÃ y giá»›i thiá»‡u vá» cÃ¡c FR algorithms trong nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y. Dá»±a trÃªn nhiá»u khÃ­a cáº¡nh cá»§a cÃ¡c FR modeling, há» chia pháº§n nÃ y thÃ nh cÃ¡c má»¥c sau.
1. designing loss function
2. refining embedding
3. FR with massive IDs
4. FR on uncommon images
5. FR pipeline acceleration
6. close-set training 

## 4.1. Loss function
Tá»•ng káº¿t: cÃ¡c phÆ°Æ¡ng phÃ¡p á»Ÿ pháº§n 4.1.1 cho ra hiá»‡u xuáº¥t tháº¥p hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng  phÃ¡p á»Ÿ pháº§n 4.1.2
PhÆ°Æ¡ng phÃ¡p máº¡nh nháº¥t lÃ  MagFace vÃ  AdaFace.

### 4.1.1. Loss based on metric learning

Má»¥c tiÃªu cá»§a pháº§n nÃ y lÃ  giá»›i thiá»‡u cÃ¡c hÃ m loss function Ä‘Æ°á»£c sá»­ dá»¥ng trong lÄ©nh vá»±c FR. CÃ¡c loss function nÃ y chÃ­nh lÃ  1 metric trong quÃ¡ trÃ¬nh training (loss bassed on metric learning lÃ  vÃ¬ váº­y). Báº£n cháº¥t model há»c dá»±a theo loss function lÃ  quÃ¡ trÃ¬nh model thiáº¿t láº­p 1 feature face, sao cho náº¿u 2 face thuá»™c cÃ¹ng 1 id khi tham chiáº¿u lÃªn feature face nÃ y, khoáº£ng cÃ¡ch euclidean/khoáº£ng cÃ¡ch cosin cá»§a chÃºng pháº£i lÃ  nhá» nháº¥t cÃ³ thá»ƒ.

1. Vá»›i pipeline cá»§a face verification (chá»‰ cáº§n xÃ¡c Ä‘á»‹nh true hay fale), *Han vÃ  cÃ¡c cÃ´ng sá»± cá»§a a*[40] sá»­ dá»¥ng **cross-entropy** lÃ m loss function trong quÃ¡ trÃ¬nh train model.
    ![](images/4.1.1.%20cross-entropy.png)
    
    yij lÃ  nhÃ£n nhá»‹ phÃ¢n xÃ¡c Ä‘á»‹nh xem 2 image i vÃ  j cÃ³ cÃ¹ng 1 ID hay khÃ´ng (yij=1 náº¿u cÃ¹ng id vÃ  = 0 náº¿u ngÆ°á»£c ID).
    pij lÃ  giÃ¡ trá»‹ logits (Ä‘áº§u ra cá»§a mÃ´ hÃ¬nh sau khi Ã¡p dá»¥ng hÃ m sigmoid) biá»ƒu thá»‹ xÃ¡c xuáº¥t mÃ  mÃ´ hÃ¬nh dá»± Ä‘oÃ¡n 2 khuÃ´n máº·t cá»§a image xem cÃ³ cÃ¹ng danh tÃ­nh hay khÃ´ng.
    Má»¥c tiÃªu cá»§a hÃ m loss nÃ y lÃ  Ä‘o lÆ°á»ng sá»± khÃ¡c biá»‡t giá»¯a pij (gÃ­a trá»‹ dá»± Ä‘oÃ¡n) vÃ  yij (giÃ¡ trá»‹ gá»‘c cá»§a label). Khi pij cÃ ng gáº§n yij thÃ¬ loss cÃ ng nhá» vÃ  ngÆ°á»£c láº¡i => Model biáº¿t ma tráº­n trá»ng sá»‘ trong epoch nhÆ° tháº¿ nÃ o vá»›i epoch trÆ°á»›c Ä‘Ã³ mÃ  cáº£i thiá»‡n.

2. **contrastive loss**
    KhÃ¡c vá»›i [40], contrastive loss Ä‘Æ°á»£c [41] Ä‘á» xuáº¥t Ä‘á»ƒ so sÃ¡nh trá»±c tiáº¿p feature cá»§a 2 face trong 2 image. Váº«n lÃ  tiÃªu chÃ­: náº¿u 2 face thuá»™c cÃ¹ng 1 ID thÃ¬ khoáº£ng cÃ¡ch giá»¯a 2 feature extract Ä‘Æ°á»£c tá»« 2 face trÃªn tham chiáº¿u lÃªn traind space pháº£i gáº§n nhau nháº¥t vÃ  ngÆ°á»£c láº¡i. Loss function contrastive nhÆ° sau.

    ![](images/4.1.1.%20contrastive.png)

    i, j lÃ  2 áº£nh chá»©a 2 khuÃ´n máº·t trong dataset vÃ  fi, fj lÃ  feature mÃ  model extract Ä‘Æ°á»£c tá»« chÃºng => ||fi-fj||^2 lÃ  khoáº£ng cÃ¡ch euclidea cá»§a chÃºng.
    m lÃ  margin (biÃªn Ä‘á»™) Ä‘á»ƒ má»Ÿ rá»™ng khoáº£ng cÃ¡ch cá»§a cÃ¡c máº«u cÃ³ ID khÃ¡c nhau (negative pairs)

    CÃ´ng thá»©c trÃªn Ä‘Æ°á»£c tÃ­nh toÃ¡n dá»±a theo khoáº£ng cÃ¡ch euclidean, ta cÃ³ thá»ƒ thay tháº¿ nÃ³ náº¿u sá»­ dá»¥ng khoáº£ng cÃ¡ch cosin.
    
    ![](images/4.1.1.%20contrastive-cosin.png)

    trong d lÃ  cosin simalarity giá»¯a 2 feature fi vÃ  fj, w vÃ  b lÃ  2 tham sá»‘ mÃ  model cÃ³ thá»ƒ há»c vÃ  tá»· lá»‡ cÃ³ thá»ƒ há»c Ä‘Æ°á»£c. phi lÃ  sigmoid fuction.

3. **BioMetricNet** [42]
    KhÃ¡c vá»›i 2 hÃ m loss trÃªn, cÃ³ thá»ƒ tháº¥y model phá»¥ thuá»™c vÃ o 1 cÃ´ng thá»©c loss xÃ¡c Ä‘á»‹nh. Tuy nhiÃªn BioMetricNet khÃ´ng sá»­ dá»¥ng metric cá»‘ Ä‘á»‹nh nhÆ° euclid hay cosin Ä‘á»ƒ so sÃ¡nh Ä‘áº·c trÆ°ng khuÃ´n máº·t. Thay vÃ o Ä‘Ã³, BioMetricNet quyáº¿t Ä‘á»‹nh training space báº±ng cÃ¡ch há»c 1 biá»ƒu diá»…n tiá»m áº©n (latent representation) mÃ  cÃ¡c positive pair vÃ  negative pair Ä‘Æ°á»£c map vÃ o cÃ¡c phÃ¢n phá»‘i má»¥c tiÃªu rÃµ rÃ ng vÃ  tÃ¡ch biá»‡t.

    QuÃ¡ trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a BioMetricNet bao gá»“m: trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng cá»§a face pair, Ã¡nh xáº¡ nÃ³ vÃ o khÃ´ng gian má»›i nÆ¡i quyáº¿t Ä‘á»‹nh 2 face cÃ³ cÃ¹ng ID hay khÃ´ng Ä‘Æ°á»£c táº¡o ra. Loss cá»§a BioMetricNet ráº¥t phá»©c táº¡p dÃ¹ng Ä‘á»ƒ Ä‘o lÆ°á»ng sá»± phÃ¢n phá»‘i xÃ¡c suáº¥t thá»‘ng kÃª cá»§a cÃ¡c positive pair vÃ  negative pair trong khÃ´ng gian nÃ y.

4. **FaceNet-Triplet loss** [34]
    FaceNet Ä‘á» xuáº¥t 1 **triplet loss**: Äáº§u vÃ o gá»“m 3 áº£nh, anchor vÃ  positive thuá»™c cÃ¹ng 1 ID vÃ  negative khÃ¡c ID. 
    Má»¥c tiÃªu cá»§a triplet loss lÃ  **Ä‘á»ƒ tá»‘i thiá»ƒu hÃ³a khoáº£ng cÃ¡ch giá»¯a anchor vÃ  positive vÃ  tá»‘i Ä‘a khoáº£ng cÃ¡ch giá»¯a áº£nh anchor vÃ  negative**.
    Äá»™ng lá»±c ra Ä‘á»i cá»§a hÃ m nÃ y lÃ  [44] trong bá»‘i cáº£nh cáº§n phÃ¢n loáº¡i nearest-neighbor. Triplet loss Ä‘áº£m báº£o cÃ¡c áº£nh x^a (anchor) cá»§a 1 ngÆ°á»i sáº½ gáº§n vá»›i táº¥t cáº£ cÃ¡c image x^p (positive) cá»§a ngÆ°á»i  Ä‘Ã³ trong dataset vÃ  xa hÆ¡n vá»›i báº¥t ká»³ x^n (negative) cá»§a báº¥t ká»³ ngÆ°á»i nÃ o khÃ¡c.

    ![](images/4.1.1.%20triplet.png)

    T cÃ³ thá»ƒ hiá»ƒu lÃ  dataset sao cho Ä‘áº£m báº£o cÃ¡c Ä‘iá»u kiá»‡n hÃ¬nh thÃ nh anchor, positive, negative 
    f(x): face embededing cá»§a x, 
    alpha: lÃ  margin (biÃªn Ä‘á»™). Cá»¥ thá»ƒ ta muá»‘n Ä‘áº£m báº£o khoáº£ng cÃ¡ch (anchor, posivte) luÃ´n nhá» hÆ¡n khoáº£ng cÃ¡ch (anchor, negative) vá»›i 1 biÃªn Ä‘á»™ nháº¥t Ä‘á»‹nh margin. ÄÃ¢y lÃ  1 tham sá»‘ cá»‘ Ä‘á»‹nh Ä‘Æ°á»£c chá»n trÆ°á»›c khi train model.
    Loss sáº½ lÃ  0 náº¿u táº¥t cáº£ bá»™ ba thuá»™c T Ä‘áº£m báº£o Ä‘iá»u trÃªn.

    Äá»ƒ Ä‘áº£m báº£o qÃºa trÃ¬nh train há»™i tá»¥ nhanh (fast convergence), ta cáº§n pháº£i nhanh chÃ³ng chá»n ra bá»™ 3 vi pháº¡m triplet loss, cÃ¹ng nhÃ¬n láº¡i Ã½ nghÄ©a cá»§a triplet loss.
    
    ![](images/4.1.1.%20triplet%20purpose.png)

    CÃ³ thá»ƒ tháº¥y triplet loss ráº¥t nháº¡y cáº£m vá»›i viá»‡c chá»n bá»™ 3 (xa,xp,xn). Thay vÃ¬ thá»­ bá»«a positve vÃ  negative, ta cá»‘ gáº¯ng chá»n xp, xn sao cho hard-positive vÃ  hard-negative xáº£y ra
        hard-positive: khoáº£ng cÃ¡ch (xp, xa) lá»›n nháº¥t => GiÃºp mÃ´ hÃ¬nh há»c cÃ¡ch thu nhá» khoáº£ng cÃ¡ch giá»¯a anchor vÃ  positive.
        hard-negative: khoáº£ng cÃ¡ch (xn,xa) nhá» nháº¥t => Nháº±m tÄƒng Ä‘á»™ khÃ³ cho mÃ´ hÃ¬nh khi cá»‘ gáº¯ng phÃ¢n biá»‡t anchor vá»›i negative.

5. Kang loss (tá»± Ä‘áº·t)
    Kang vÃ  cÃ¡c cá»™ng sá»± cá»§a anh áº¥y[44] Ä‘Ã£ Ä‘Æ¡n giáº£n hÃ³a contrastive vÃ  triplet loss vÃ  thiáº¿t káº¿ 1 cÃ´ng thá»©c loss má»›i.

    ![](images/4.1.1.%20kang%20loss.png)

    trong Ä‘Ã³ ![](images/4.1.1.%20kang%20loss%20explain.png)

    Tá»•ng há»£p láº¡i, hÃ m máº¥t mÃ¡t nÃ y giÃºp mÃ´ hÃ¬nh há»c nhanh vÃ  hiá»‡u quáº£ hÆ¡n báº±ng cÃ¡ch táº­p trung vÃ o cÃ¡c vÃ­ dá»¥ khÃ³ (hard examples) vÃ  cáº£i thiá»‡n sá»± phÃ¢n tÃ¡ch giá»¯a cÃ¡c danh tÃ­nh khÃ¡c nhau trong khÃ´ng gian Ä‘áº·c trÆ°ng.

Tá»•ng há»£p láº¡i: Contrastive loss dÃ¹ng Ä‘á»ƒ Ä‘o khoáº£ng cÃ¡ch giá»¯a cÃ¡c image pair, triplet loss Ä‘áº£m báº£o má»‘i quan há»‡ giá»¯a bá»™ 3 sample (anchor, positive vÃ  negative). NgoÃ i ra cÃ²n ráº¥t nhiá»u loss thuá»™ dáº¡ng metric learning Ä‘Æ°á»£c cÃ¡c researcher phÃ¡t triá»ƒn báº±ng cÃ¡ch Ä‘Æ°a nhiá»u sample hÆ¡n trong mÃ´ táº£ hÃ m loss.

6. center loss
    Dung Ä‘á»ƒ giáº£m thiá»ƒu sá»± phÃ¢n tÃ¡n cá»§a cÃ¡c feature trong cÃ¹ng 1 class báº±ng cÃ¡ch Ä‘áº·t chÃºng vá» gáº§n trung tÃ¢m cá»§a class. Center loss thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng vá»›i softmax Ä‘á»ƒ tÄƒng tÃ­nh phÃ¢n biá»‡t giá»¯a cÃ¡c ID (refine - tinh chá»‰nh). CÃ´ng thá»©c center loss nhÆ° sau.

        ![](images/4.1.1.%20center.png)

    Lsoftmax dá»±a trÃªn label cá»§a training data, i lÃ  image dÃ¹ng Ä‘á»ƒ train vá»›i nhÃ£n tháº­t lÃ  yi, xi lÃ  deep feature, m lÃ  sá»‘ lÆ°á»£ng training class, cij biá»ƒu thá»‹ trung tÃ¢m cá»§a class chá»©a áº£nh yi

### 4.1.2. Larger margin loss
BÃ i toÃ¡n FR cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  classification problem vÃ  sá»­ dá»¥ng softmax Ä‘á»ƒ train model. Larger margin loss (hay tá»•n tháº¥t dá»±a trÃªn biÃªn Ä‘á»™ gÃ³c - angular margin bassed loss) Ä‘Æ°á»£c láº¥y cáº£m há»©ng tá»« hÃ m softmax. Dáº¡ng loss func nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng phá»• biáº¿n trong lÄ©nh vá»±c FR trong há»c táº­p vÃ  cÃ´ng nghiá»‡p. NÃ³ cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ hiá»‡u xuáº¥t cá»§a mÃ´ hÃ¬nh FR.

Báº£n cháº¥t ta váº«n muá»‘n cacs facial images (Ä‘áº·c tÃ­nh khuÃ´n máº·t) cá»§a cÃ¡c máº·t cÃ³ cÃ¹ng ID sáº½ gáº§n nhau hÆ¡n trong khÃ´ng gian train vÃ  cÃ¡c áº£nh khÃ¡c ID sáº½ xa nhau trong khÃ´ng gian train. Káº¿t quáº£ large margin loss khuyáº¿n khÃ­ch sá»± Ä‘á»“ng nháº¥t trong 1 lá»›p (intra-class compactness) vÃ  trá»«ng pháº¡t sá»± Ä‘á»“ng nháº¥t cá»§a cÃ¡c áº£nh khÃ¡c ID - hay sá»± khÃ¡c biá»‡t giá»¯a cÃ¡c lá»›p (inter-class separability).

CÃ´ng thá»©c softmax, cÃ¡c khÃ¡i niá»‡m sau Ä‘Ã¢y sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng  á»Ÿ toÃ n bá»™ pháº§n nÃ y:
    ![](images/4.1.2.%20softmax.png)

    W lÃ  ma tráº­n trá»ng sá»‘ thu Ä‘Æ°á»£c sau last fully connected layer vÃ  Ä‘Æ°á»£c sá»­ dá»¥ng tÃ­nh loss trong quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c.
    Wj Ä‘áº¡i diá»‡n cho weight cá»§a class j
    xi, yi: lÃ  face embedding cá»§a áº£nh i vÃ  group id thá»±c táº¿ cá»§a áº£nh i. Trong class yi, áº£nh xi cÃ³ vai trÃ² positive vÃ  vá»›i class yj (j#i) thÃ¬ áº£nh xi cÃ³ vai trÃ² negative. 

**Softmax truyá»n thá»‘ng chá»‰ Ä‘Æ¡n thuáº§n xÃ¡c Ä‘á»‹nh nhÃ£n cho má»—i máº«u mÃ  khÃ´ng tá»‘i Æ°u hÃ³a khoáº£ng cÃ¡ch giá»¯a cÃ¡c danh tÃ­nh trong khÃ´ng gian biá»ƒu diá»…n. Tuy nhiÃªn nÃ³ lÃ  ná»n táº£ng Ä‘á»ƒ má»Ÿ rá»™ng cÃ¡c hÃ m loss á»Ÿ phaafnnafy**

Dá»±a trÃªn positive sample vÃ  negative sample, softmax cÃ³ thá»ƒ Ä‘Æ°á»£c viáº¿t láº¡i thÃ nh.
    ![](images/4.1.2.%20softmax-positve,negative.png)

1. **L-softmax** [46] Ä‘Ã¢y lÃ  loss function Ä‘áº§u tiÃªn thiáº¿t káº¿ theo margin bassed loss báº±ng cÃ¡ch Ä‘o features angles (gÃ³c cá»§a cÃ¡c Ä‘áº·c Ä‘iá»ƒm).
    Äáº§u tiÃªn nÃ³ bá» qua bias cá»§a má»—i class bj vÃ  chuyá»ƒn tÃ­ch Wj * xi thÃ nh â€–Wjâ€–Â·â€–xiâ€–Â·cos(Î¸j) trong Ä‘Ã³ Î¸j lÃ  gÃ³c giá»¯a xi vÃ  ma tráº­n trá»ng sá»‘ Wj cá»§a class j.
    Äá»ƒ má»Ÿ rá»™ng biÃªn Ä‘á»™ gÃ³c giá»¯a cÃ¡c lá»›p, L-softmax biáº¿n Ä‘á»•i cos(Î¸yi) thÃ nh Ïˆ(Î¸yi) báº±ng cÃ¡ch thu háº¹p khoáº£ng cÃ¡ch boundary (viá»n ngoÃ i class) vá»›i trung tÃ¢m class (center class)

    ![](images/4.1.2.%20L-softmax.png)

    m lÃ  1 tham sá»‘ integer cá»‘ Ä‘á»‹nh , gÃ³c Î¸ Ä‘Æ°á»£c chia thÃ nh m Ä‘oáº¡n  [kÏ€/m , (k+1)Ï€/m] ....

2. **A-softmax**: ÄÆ°á»£c giá»›i thiá»‡u trong Sphereface [47]
    LÃ  báº£n cáº£i tiáº¿n L-softmax báº±ng cÃ¡ch chuáº©n hÃ³a trá»ng sá»‘ cá»§a tá»«ng class (Wj) trÆ°á»›c khi tÃ­nh loss  vÃ  káº¿t há»£p vá»›i L-softmax Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c khi train. Loss trá»Ÿ thÃ nh

    ![](images/4.1.2.%20A-softmax.png)

    Trong quÃ¡ trÃ¬nh train máº¡nh sphereface, thá»±c táº¿ loss láº¡i lÃ : (1-Î±)*softmax + Î±*A-softmax vá»›i Î± thuá»™c Ä‘oáº¡n [0,1] vÃ  thay Ä‘á»•i trong quÃ¡ trÃ¬nh train. Thiáº¿t káº¿ nÃ y cÃ³ 2 má»¥c Ä‘Ã­ch
        Náº¿u sá»­ dá»¥ng A-softmax lÃ m loss function trá»±c tiáº¿p sáº½ dáº«n Ä‘áº¿n hard convergence (há»™i tá»¥ cá»©ng) vÃ¬ nÃ³ Ä‘áº©y máº¡nh cÃ¡c feature cá»§a cÃ¡c IDs khÃ¡c ra xa
        Náº¿u train vá»›i softmax trÆ°á»›c sáº½ giáº£m angle Î¸yi giá»¯a feature i vÃ  trá»ng sá»‘ Wyi liÃªn quan Ä‘áº¿n nÃ³. => Cos(mÎ¸) tribg A-softmax sáº½ náº±m trong 1 vÃ¹ng Ä‘Æ¡n Ä‘iá»‡u vÃ  dá»… dÃ ng Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c 1 loss tháº¥p trong quÃ¡ trÃ¬nh gradient decending.

3. **Normface loss**
    BÃªn cáº¡nh viá»‡c normalize weight cá»§a má»—i class, NormFace [48] Ä‘á» xuáº¥t normalize cáº£ face embedding, sau Ä‘Ã³ scale up normalized face embedding báº±ng 1 tham sá»‘ tá»· lá»‡ s. Äiá»u nÃ y sáº½ lÃ m giáº£m váº¥n Ä‘á» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u giá»¯a positive vÃ  negative sample vÃ  cÃ³ sá»± há»™ tá»¥ tá»‘t hÆ¡n. Tuy nhiÃªn Normface khÃ´ng sá»­ dá»¥ng margin nhÆ° Sphereface Ä‘á»‘i vá»›i positive sample. CÃ´ng thá»©c NormFace loss nhÆ° sau:
    
    ![](images/4.1.2.%20Normface.png)

    W~vÃ  x~ á»Ÿ trÃªn lÃ  normalized class vÃ  normalized face embedding
    s lÃ  tham sá»‘ scale up (s>1)

    **Äiá»u nÃ y giÃºp cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t Ä‘Æ°á»£c phÃ¢n bá»‘ Ä‘á»u trÃªn má»™t hypersphere, lÃ m tÄƒng kháº£ nÄƒng phÃ¢n biá»‡t cá»§a mÃ´ hÃ¬nh mÃ  khÃ´ng cáº§n thÃªm margin.**

4. **AM-softmax[49]** vÃ  **CosFace[50]**
    Há» má»Ÿ rá»™ng hÃ m softmax cÆ¡ báº£n báº±ng cÃ¡ch trá»« Ä‘i margin m bÃªn ngoÃ i cos(Î¸). CÃ´ng thá»©c nhÆ° sau
    ![](images/4.1.2.%20cosface.png)

5. **ArcFace[51]**
    ÄÆ°a margin vÃ o gÃ³c trong cos(Î¸) vÃ  lÃ m margin dá»… lÃ½ giáº£i hÆ¡n

    ![](images/4.1.2.%20arcface.png)

6. P2SGrad [52]
    **CÃ¡c tá»•n tháº¥t dá»±a trÃªn angular margiin bassed loss á»Ÿ trÃªn (CosFace vÃ  ArcFace) bao gá»“m cÃ¡c hyper-parameters nháº¡y cáº£m. Äiá»u nÃ y cÃ³ thá»ƒ lÃ m cho quÃ¡ trÃ¬nh train khÃ´ng á»•n Ä‘á»‹nh. Báº£n cháº¥t cÃ¡c hÃ m CosFace vÃ  ArcFace lÃ  cÃ¡c hÃ m máº¥t mÃ¡t thÃªm margin gÃ³c cho cÃ¡c positive sample Ä‘á»ƒ tÄƒng cÆ°á»ng kháº£ nÄƒng phÃ¢n biá»‡t. Tuy nhiÃªn chÃºng cÃ³ 2 háº¡n cháº¿.**
        Nháº¡y cáº£m vá»›i cÃ¡c siÃªu tham sá»‘: CosFace vÃ  ArcFace cáº§n cÃ¡c tham sá»‘ nhÆ° margin ğ‘š vÃ  scale s, Ä‘iá»u nÃ y cÃ³ thá»ƒ lÃ m cho quÃ¡ trÃ¬nh huáº¥n luyá»‡n khÃ´ng á»•n Ä‘á»‹nh vÃ  khÃ³ há»™i tá»¥.
        Chá»‰ tÃ¡c Ä‘á»™ng lÃªn máº«u dÆ°Æ¡ng: Cáº£ CosFace vÃ  ArcFace chá»‰ thÃªm margin cho cÃ¡c máº«u dÆ°Æ¡ng, do Ä‘Ã³ chá»‰ Ä‘áº£m báº£o ráº±ng cÃ¡c máº«u dÆ°Æ¡ng sáº½ gáº§n vá»›i trung tÃ¢m lá»›p cá»§a chÃºng mÃ  khÃ´ng tá»‘i Æ°u hÃ³a khoáº£ng cÃ¡ch giá»¯a cÃ¡c máº«u Ã¢m (negative samples) vÃ  lá»›p dÆ°Æ¡ng.

    P2SGrad [52] Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£i quyáº¿t thÃ¡ch thá»©c nÃ y báº±ng cÃ¡ch trá»±c tiáº¿p thiáº¿t kÃª cÃ¡c gradient cho cÃ¡c máº«u theo cÃ¡ch thÃ­ch á»©ng. Thay vÃ¬ sá»­ dá»¥ng cÃ¡c margin cá»‘ Ä‘á»‹nh, P2SGrad Ä‘iá»u chá»‰nh gradient dá»±a trÃªn cÃ¡c máº«u Ä‘áº§u vÃ o, giÃºp mÃ´ hÃ¬nh linh hoáº¡t hÆ¡n trong viá»‡c há»c vÃ  tÄƒng kháº£ nÄƒng há»™i tá»¥.

7. SV-Softmax[53] vÃ  cÃ¡c biáº¿n thá»ƒ
    ÄÃ¢y lÃ  1 cáº£i tiáº¿n khÃ¡c so vá»›i AM-softmax, CosFace vÃ  Arcface. NÃ³ khÃ´ng chá»‰ táº­p trung vÃ o positive samples mÃ  cÃ²n tÃ¡c Ä‘á»™ng Ä‘áº¿n negative samples.
    Thay vÃ¬ chá»‰ Ä‘Æ°a cÃ¡c máº«u dÆ°Æ¡ng láº¡i gáº§n trung tÃ¢m lá»›p, SV-Softmax Ä‘áº©y cÃ¡c máº«u Ã¢m khÃ³ (hard negative samples) ra xa trung tÃ¢m lá»›p cá»§a cÃ¡c máº«u dÆ°Æ¡ng.
    CÃ¡ch nÃ y Ä‘áº£m báº£o ráº±ng cÃ¡c lá»›p sáº½ tÃ¡ch biá»‡t hÆ¡n, giáº£m thiá»ƒu viá»‡c cÃ¡c máº«u Ã¢m xÃ¢m nháº­p vÃ o khÃ´ng gian cá»§a lá»›p dÆ°Æ¡ng, tá»« Ä‘Ã³ tÄƒng kháº£ nÄƒng phÃ¢n biá»‡t cá»§a mÃ´ hÃ¬nh.
    Äá»ƒ chá»n cÃ¡c máº«u Ã¢m khÃ³, SV-Softmax sá»­ dá»¥ng nhÃ£n nhá»‹ phÃ¢n ğ¼ğ‘—, chá»‰ Ä‘á»‹nh xem má»™t máº«u ğ‘— cÃ³ pháº£i lÃ  hard negative khÃ´ng, dá»±a vÃ o Ä‘iá»u kiá»‡n:

    ![](images/4.1.2.%20L-softmax.png)
    
    SV-X-Softmax lÃ  phiÃªn báº£n má»Ÿ rá»™ng cá»§a SV-Softmax báº±ng cÃ¡ch bá»• sung large margin cho cÃ¡c máº«u dÆ°Æ¡ng. HÃ m máº¥t mÃ¡t Ä‘Æ°á»£c Ä‘iá»u chá»‰nh nhÆ° sau:
    
    ![](images/4.1.2.%20SV-X-softmax.png)

    MV-Softmax lÃ  bÆ°á»›c tiáº¿n hÃ³a tiáº¿p theo cá»§a SV-Softmax, trong Ä‘Ã³ hÃ m â„(ğ‘¡,ğœƒğ‘—,ğ¼ğ‘—) Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a láº¡i Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng Ä‘iá»u chá»‰nh trá»ng sá»‘ cá»§a cÃ¡c máº«u Ã¢m. Thay Ä‘á»•i nÃ y giÃºp MV-Softmax linh hoáº¡t hÆ¡n trong viá»‡c xÃ¡c Ä‘á»‹nh vÃ  xá»­ lÃ½ cÃ¡c máº«u Ã¢m khÃ³, tÄƒng Ä‘á»™ chÃ­nh xÃ¡c vÃ  á»•n Ä‘á»‹nh trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

    TÃ³m láº¡i: SV-Softmax vÃ  cÃ¡c biáº¿n thá»ƒ cá»§a nÃ³ nhÆ° SV-X-Softmax vÃ  MV-Softmax cáº£i thiá»‡n sá»± phÃ¢n biá»‡t cá»§a mÃ´ hÃ¬nh báº±ng cÃ¡ch tá»‘i Æ°u hÃ³a cÃ¡c máº«u Ã¢m khÃ³ vÃ  tÄƒng khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p dÆ°Æ¡ng vÃ  Ã¢m, giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch phÃ¢n loáº¡i chÃ­nh xÃ¡c hÆ¡n.

8. Ring loss[55]
    CosFace [50] and ArcFace [51] thá»±c hiá»‡n chuáº©n hÃ³a loss. Ring Loss lÃ  má»™t hÃ m máº¥t mÃ¡t Ä‘áº·c biá»‡t nháº±m chuáº©n hÃ³a Ä‘á»™ dÃ i cá»§a cÃ¡c embedding Ä‘áº·c trÆ°ng khuÃ´n máº·t vá» má»™t giÃ¡ trá»‹ cá»‘ Ä‘á»‹nh ğ‘…. Äiá»u nÃ y giÃºp cÃ¡c embedding cÃ³ cÃ¹ng Ä‘á»™ dÃ i, tÄƒng Ä‘á»™ á»•n Ä‘á»‹nh vÃ  giÃºp mÃ´ hÃ¬nh há»™i tá»¥ tá»‘t hÆ¡n khi huáº¥n luyá»‡n. CÃ´ng thá»©c Ring Loss:
        ![](images/4.1.2.%20Ringloss.png)
        m lÃ  batch size
        Î» lÃ  trá»ng sá»‘ Ä‘á»ƒ Ä‘Ã¡nh Ä‘á»•i giá»¯a hÃ m máº¥t mÃ¡t chÃ­nh. Trong [55], hÃ m máº¥t mÃ¡t chÃ­nh Ä‘Æ°á»£c Ä‘áº·t thÃ nh softmax vÃ  SphereFace [47].
    
9. **Trong trÆ°á»ng há»£p training set cÃ³ cÃ¡c áº£nh bá»‹ noise**, Hu et al[56] Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p há»c vá»›i dá»¯ liá»‡u nhiá»…u, trong Ä‘Ã³ **gÃ³c ğœƒ giá»¯a máº«u vÃ  trung tÃ¢m lá»›p tÆ°Æ¡ng á»©ng Ä‘Ã³ng vai trÃ² Ä‘Ã¡nh giÃ¡ Ä‘á»™ nhiá»…u cá»§a máº«u**. Máº«u cÃ³ gÃ³c ğœƒ nhá» hÆ¡n sáº½ Ã­t nhiá»…u hÆ¡n vÃ  Ä‘Æ°á»£c gÃ¡n trá»ng sá»‘ cao hÆ¡n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n Ä‘á»ƒ mÃ´ hÃ¬nh Æ°u tiÃªn há»c tá»« chÃºng. 
    PhÃ¢n phá»‘i cá»§a gÃ³c ğœƒ:
        Trong má»™t táº­p training set chá»©a nhiá»…u, gÃ³c ğœƒ thÆ°á»ng cÃ³ dáº¡ng phÃ¢n phá»‘i Gaussian vá»›i hai cá»±c trá»‹. Hai cá»±c trá»‹ nÃ y tÆ°Æ¡ng á»©ng vá»›i cÃ¡c máº«u nhiá»…u vÃ  máº«u sáº¡ch.
        PhÃ¢n phá»‘i cá»§a ğœƒ cÃ³ cÃ¡c Ä‘iá»ƒm cá»±c trÃ¡i (ğ›¿ğ‘™) vÃ  cá»±c pháº£i (ğ›¿ğ‘Ÿ), cÃ¹ng vá»›i cÃ¡c Ä‘á»‰nh phÃ¢n phá»‘i Gaussian á»Ÿ ğœ‡ğ‘™ vÃ  ğœ‡ğ‘Ÿ (náº¿u chá»‰ cÃ³ má»™t Gaussian, ğœ‡ğ‘™=ğœ‡ğ‘Ÿ).
    
    ![](images/4.1.2.%20Ringloss%20AM-softmax.png)

    ![](images/4.1.2.%20Ringloss%20AM-softmax-2.png)

10. Sub-center ArcFace vÃ  CurricularFace: hai phÆ°Æ¡ng phÃ¡p cáº£i tiáº¿n cá»§a ArcFace nháº±m giáº£i quyáº¿t váº¥n Ä‘á» dá»¯ liá»‡u nhiá»…u vÃ  tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t mÃ´ hÃ¬nh nháº­n diá»‡n khuÃ´n máº·t.
    ![](images/4.1.2.%20Sub-center%20Arceface.png)

    ![](images/4.1.2.%20CuricularFace.png)

    alpha lÃ  momemtem 

11. NPCFace[59]:
    NPCface â€“ má»™t ká»¹ thuáº­t sá»­ dá»¥ng trong huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh nháº­n diá»‡n khuÃ´n máº·t Ä‘á»ƒ xá»­ lÃ½ cÃ¡c trÆ°á»ng há»£p khÃ³ (hard cases) má»™t cÃ¡ch hiá»‡u quáº£. Ã tÆ°á»Ÿng chÃ­nh cá»§a NPCface lÃ  táº­p trung vÃ o nhá»¯ng máº«u dá»¯ liá»‡u khÃ³ Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh.

    Má»¥c tiÃªu cá»§a NPCface lÃ  nháº¥n máº¡nh viá»‡c huáº¥n luyá»‡n trÃªn cÃ¡c máº«u khÃ³ báº±ng cÃ¡ch Ä‘iá»u chá»‰nh margin dá»±a trÃªn Ä‘á»™ khÃ³ cá»§a cÃ¡c máº«u, tá»« Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng phÃ¢n loáº¡i cá»§a mÃ´ hÃ¬nh trong cÃ¡c táº­p dá»¯ liá»‡u lá»›n vÃ  Ä‘a dáº¡ng.

    ![](images/4.1.2.%20NPCface.png)

12. UniformFace[60] nháº±m tá»‘i Æ°u hÃ³a sá»± phÃ¢n bá»‘ cá»§a cÃ¡c lá»›p trÃªn manifold (Ä‘a táº¡p) dáº¡ng hypersphere Ä‘á»ƒ Ä‘áº¡t sá»± Ä‘á»“ng Ä‘á»u vÃ  tá»‘i Ä‘a hÃ³a khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p, giÃºp cáº£i thiá»‡n hiá»‡u quáº£ phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p khuÃ´n máº·t.

    UniformFace nháº­n xÃ©t ráº±ng cÃ¡c hÃ m large margin loss nhÆ° CosFace vÃ  ArcFace chÆ°a cÃ¢n nháº¯c Ä‘áº¿n sá»± phÃ¢n bá»‘ cá»§a táº¥t cáº£ cÃ¡c lá»›p. CÃ¡c lá»›p cÃ³ thá»ƒ khÃ´ng Ä‘Æ°á»£c phÃ¢n bá»‘ Ä‘á»u trÃªn manifold, dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh khÃ³ khÄƒn khi phÃ¢n biá»‡t cÃ¡c lá»›p tÆ°Æ¡ng tá»± nhau.

    Vá»›i quan Ä‘iá»ƒm ráº±ng cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t náº±m trÃªn manifold dáº¡ng hypersphere, UniformFace Ã¡p dá»¥ng rÃ ng buá»™c equidistributed (phÃ¢n bá»‘ Ä‘á»u), cá»‘ gáº¯ng tá»‘i Ä‘a hÃ³a khoáº£ng cÃ¡ch tá»‘i thiá»ƒu giá»¯a cÃ¡c center (tÃ¢m) cá»§a cÃ¡c lá»›p, giÃºp táº­n dá»¥ng tá»‘i Ä‘a khÃ´ng gian Ä‘áº·c trÆ°ng.

    ![](images/4.1.2.%20UniformFace'.png)

13. RegularFace[61]
    Zhao et al. (RegularFace) má»Ÿ rá»™ng khÃ¡i niá»‡m â€œinter-class separabilityâ€ (kháº£ nÄƒng tÃ¡ch biá»‡t giá»¯a cÃ¡c lá»›p) vÃ o cÃ¡c hÃ m loss dá»±a trÃªn large-margin Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c. RegularFace Ä‘o lÆ°á»ng khoáº£ng cÃ¡ch gÃ³c (cosine similarity) lá»›n nháº¥t giá»¯a má»™t lá»›p ğ‘– vÃ  cÃ¡c lá»›p khÃ¡c vá»›i cÃ´ng thá»©c:

    ![](images/4.1.2.%20RegularFace.png)

14. Variational Prototype Learning (VPL)
    VPL má»Ÿ rá»™ng viá»‡c Ä‘o lÆ°á»ng khoáº£ng cÃ¡ch tá»« máº«u tá»›i tÃ¢m lá»›p (sample-to-prototype) báº±ng cÃ¡ch sá»­ dá»¥ng khoáº£ng cÃ¡ch tá»« máº«u Ä‘áº¿n prototype biáº¿n thiÃªn (variational prototype), giÃºp cáº£i thiá»‡n tÃ­nh linh hoáº¡t cá»§a mÃ´ hÃ¬nh.

    ![](images/4.1.2%20Variational%20Prototype%20Learning.png)

15. UIR (Unlabeled ID Regularization)
    UIR huáº¥n luyá»‡n bá»™ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng trong mÃ´i trÆ°á»ng bÃ¡n giÃ¡m sÃ¡t, báº±ng cÃ¡ch Ä‘Æ°a thÃªm cÃ¡c dá»¯ liá»‡u khÃ´ng nhÃ£n vÃ o huáº¥n luyá»‡n. => **KhÃ´ng dÃ¹ng**

    ![](images/4.1.2.%20URI.png)

CÃ¡c phÆ°Æ¡ng phÃ¡p RegularFace, VPL, vÃ  UIR Ä‘á»u nháº¯m Ä‘áº¿n viá»‡c tá»‘i Æ°u hÃ³a kháº£ nÄƒng tÃ¡ch biá»‡t giá»¯a cÃ¡c lá»›p trong khÃ´ng gian Ä‘áº·c trÆ°ng, nhÆ°ng má»—i phÆ°Æ¡ng phÃ¡p láº¡i sá»­ dá»¥ng cÃ¡c chiáº¿n lÆ°á»£c khÃ¡c nhau Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘iá»u nÃ y. RegularFace táº­p trung vÃ o viá»‡c lÃ m Ä‘á»u cÃ¡c khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p, VPL má»Ÿ rá»™ng viá»‡c Ä‘o lÆ°á»ng khoáº£ng cÃ¡ch vá»›i prototype biáº¿n thiÃªn, vÃ  UIR táº­n dá»¥ng dá»¯ liá»‡u khÃ´ng nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n phÃ¢n bá»‘ cá»§a cÃ¡c lá»›p.

16. AdaCos
    AdaCos lÃ  má»™t biáº¿n thá»ƒ cá»§a CosFace, cÃ³ kháº£ nÄƒng tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh tham sá»‘ scale ğ‘  trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n. AdaCos nháº­n tháº¥y ráº±ng náº¿u ğ‘  quÃ¡ nhá», xÃ¡c suáº¥t cá»§a má»™t máº«u sáº½ tháº¥p, ngay cáº£ khi khoáº£ng cÃ¡ch gÃ³c giá»¯a máº«u vÃ  tÃ¢m lá»›p cá»§a nÃ³ lÃ  nhá». NgÆ°á»£c láº¡i, náº¿u ğ‘  quÃ¡ lá»›n, xÃ¡c suáº¥t sáº½ Ä‘áº¡t gáº§n 1 dÃ¹ khoáº£ng cÃ¡ch gÃ³c lá»›n.

    ![](images/4.1.2.%20Adacos.png)

17. FairCos
    Fair Loss cÅ©ng Ä‘iá»u chá»‰nh tham sá»‘ margin ğ‘š trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nhÆ°ng sá»­ dá»¥ng há»c tÄƒng cÆ°á»ng (reinforcement learning). GiÃ¡ trá»‹ cá»§a ğ‘š Ä‘Æ°á»£c Ä‘iá»u chá»‰nh theo tráº¡ng thÃ¡i cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n, nháº±m tá»‘i Æ°u hÃ³a kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a cÃ¡c lá»›p.

    ![](images/4.1.2.Fair%20Loss.png)
    
18. AdaptiveFace (AdaM-Softmax)
    AdaptiveFace giáº£i quyáº¿t váº¥n Ä‘á» máº¥t cÃ¢n báº±ng dá»¯ liá»‡u giá»¯a cÃ¡c lá»›p khÃ¡c nhau, khi mÃ  má»™t sá»‘ ID cÃ³ Ã­t máº«u vÃ  sá»‘ khÃ¡c cÃ³ nhiá»u máº«u. Thay vÃ¬ sá»­ dá»¥ng margin cá»‘ Ä‘á»‹nh cho má»i lá»›p, AdaptiveFace Ä‘iá»u chá»‰nh margin ğ‘š dá»±a trÃªn sá»‘ lÆ°á»£ng máº«u cá»§a tá»«ng lá»›p.

    ![](images/4.1.2.%20AdaptiveFace-2.png)

    ![](images/4.1.2.%20AdaptiveFace.png)

Äoáº¡n vÄƒn sau tháº£o luáº­n vá» cÃ¡c cáº£i tiáº¿n trong hÃ m loss Ä‘á»ƒ tá»‘i Æ°u hÃ³a quÃ¡ trÃ¬nh há»c trong nháº­n diá»‡n khuÃ´n máº·t, Ä‘áº·c biá»‡t lÃ  viá»‡c xá»­ lÃ½ cÃ¡c máº«u khÃ³ (hard samples) vÃ  cáº£i thiá»‡n cháº¥t lÆ°á»£ng cá»§a biá»ƒu diá»…n Ä‘áº·c trÆ°ng. DÆ°á»›i Ä‘Ã¢y lÃ  tÃ³m táº¯t cÃ¹ng vá»›i cÃ¡c cÃ´ng thá»©c liÃªn quan:

19. Distribution Distillation Loss (DDL)
    Huang et al. nháº­n tháº¥y ráº±ng cÃ¡c hÃ m loss sá»­ dá»¥ng margin lá»›n thÆ°á»ng gáº·p khÃ³ khÄƒn vá»›i cÃ¡c máº«u khÃ³. Há» sá»­ dá»¥ng ArcFace Ä‘á»ƒ táº¡o ra má»™t phÃ¢n phá»‘i tá»« cÃ¡c máº«u dá»… (teacher) vÃ  má»™t phÃ¢n phá»‘i tá»« cÃ¡c máº«u khÃ³ (student). DDL Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ buá»™c phÃ¢n phá»‘i cá»§a máº«u khÃ³ gáº§n vá»›i phÃ¢n phá»‘i máº«u dá»…

    ![](images/4.1.2.%20DDL.png)

20. MagFace
    MagFace giá»›i thiá»‡u má»™t cÆ¡ cháº¿ há»c Ä‘á»ƒ phÃ¢n phá»‘i Ä‘áº·c trÆ°ng trong lá»›p tá»‘t hÆ¡n báº±ng cÃ¡ch kÃ©o cÃ¡c máº«u dá»… gáº§n tÃ¢m lá»›p vá»›i Ä‘á»™ lá»›n lá»›n hÆ¡n, trong khi Ä‘áº©y cÃ¡c máº«u khÃ³ ra xa vá»›i Ä‘á»™ lá»›n nhá» hÆ¡n.

    ![](images/4.1.2.%20MagFace.png)

21. **AdaFace**: Tháº±ng loss máº¡nh nháº¥t trong bÃ i research nÃ y Ä‘á» cáº­p
    Ã tÆ°á»Ÿng chÃ­nh: AdaFace Ä‘iá»u chá»‰nh gradient trong quÃ¡ trÃ¬nh lan truyá»n ngÆ°á»£c dá»±a trÃªn cháº¥t lÆ°á»£ng cá»§a hÃ¬nh áº£nh. CÃ¡c máº«u khÃ³ Ä‘Æ°á»£c nháº¥n máº¡nh khi cháº¥t lÆ°á»£ng hÃ¬nh áº£nh cao, vÃ  ngÆ°á»£c láº¡i.

    ![](images/4.1.2.%20AdaFace.png)

### 4.1.3. FR in unbalanced training data

Trong 1 Large-scale face dataset, khÃ´ng thá»ƒ trÃ¡nh khá»i viá»‡c cÃ³ 1 sá»‘ lá»›p cÃ³ lÆ°á»£ng dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng. Nhá»¯ng feature thuá»™c cÃ¡c class nÃ y (khÃ´ng chiáº¿m Æ°u tháº¿ - non-dominate IDs) sáº½ Ä‘Æ°á»£c nÃ©n vÃ o 1 khu vá»±c trÃªn hypershere. Äiá»u nÃ y dáº«n Ä‘áº¿n 1 sá»‘ váº¥n Ä‘á» trong quÃ¡ trÃ¬nh training. Do Ä‘Ã³, Ä‘á»‘i vá»›i cÃ¡c hiá»‡n tÆ°á»£ng máº¥t cÃ¢n báº±ng dá»¯ liá»‡u khÃ¡c nhau, cÃ¡c phÆ°Æ¡ng phÃ¡p khÃ¡c nhau Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t.

1. **Long tail distributed - PhÃ¢n phá»‘i Ä‘uÃ´i dÃ i**:
    LÃ  hiá»‡n tÆ°á»£ng máº¥t cÃ¢n báº±ng dá»¯ liá»‡u Ä‘áº§u tiÃªn, tá»“n táº¡i rá»™ng rÃ£i trÃªn Ä‘a sá»‘ cÃ¡c mainstream training set (táº­p dá»¯ liá»‡u Ä‘Ã o táº¡o chÃ­nh thá»‘ng) nhÆ° MS-Celeb-1M.

    Trong MS-Celeb-1M, háº§u nhÆ° táº¥t cáº£ cÃ¡c IDs Ä‘á»u cÃ³ sá»‘ lÆ°á»£ng áº£nh Ã­t vÃ  chá»‰ cÃ³ pháº§n nhá» sá»‘ IDs cÃ³ lÆ°á»£ng lá»›n áº£nh.

    Zhang et al. [73] Ä‘Ã£ táº¡o ra ráº¥t nhiá»u experiment Ä‘á»ƒ chá»©ng minh ráº±ng vá»›i táº¥t cáº£ tail data trong quÃ¡ trÃ¬nh training khÃ´ng thá»ƒ giÃºp model há»c tá»‘t hÆ¡n báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c loss function truyá»n thá»‘ng nhÆ° contrastive loss [41], triplet loss[34], and center loss [45]. Do Ä‘Ã³ loss funciton cáº§n Ä‘Æ°á»£c thiáº¿t káº¿ 1 cÃ¡ch tinh táº¿.

    Láº¥y cáº£m há»©ng tá»« contrastive loss, range loss [73] Ä‘Æ°á»£c táº¡o ra nháº±m giáº£m thiá»ƒu sá»± biáº¿n Ä‘á»™ng trong cÃ¹ng 1 lá»›p (intra-class variation) vÃ  Ä‘á»“ng thá»i tÄƒng cÆ°á»ng khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p khÃ¡c nhau (inter-class separation) trong hypershere.

        ![](images/4.1.3.%20Range%20loss%201.png)

        ![](images/4.1.3.%20Range%20loss%202.png)

    Zhong et al. [74] thÃ¬ Ã¡p dá»¥ng **Noise Resistance (NR) Loss** - 1 based on large margin loss Ä‘á»ƒ train vá»›i cÃ¡c dá»¯ liá»‡u Ä‘áº§u tiÃªn nháº±m xá»­ lÃ½ dá»¯ liá»‡u nhiá»…u vÃ  tá»‘i Æ°u hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng trong cÃ¡c táº­p dá»¯ liá»‡u khÃ´ng cÃ¢n báº±ng.
    
        ![](images/4.1.3.%20Noise%20Resistance.png)
    
    Sau Ä‘Ã³ sá»­ dá»¥ng **Center-Dispersed Loss (CD Loss)**
    
        ![](images/4.1.3.%20CD%20loss.png)

2. **shallow data - dá»¯ liá»‡u nÃ´ng**. ÄÃ¢y lÃ  tÃ¬nh huá»‘ng mÃ  dá»¯ liá»‡u huáº¥n luyá»‡n bá»‹ háº¡n cháº¿ vá» sá»‘ lÆ°á»£ng áº£nh cho má»—i ID (danh tÃ­nh), khiáº¿n cho nhiá»u ID chá»‰ cÃ³ má»™t sá»‘ Ã­t máº«u.
    Trong ráº¥t nhiá»u cÃ¡c ká»‹ch báº£n FR trong thá»±c táº¿, trainging dataset bá»‹ giá»›i háº¡n vá» Ä‘á»™ sÃ¢u. Chá»‰ 1 sá»‘ nhá» sample kháº£ thi trong háº§u háº¿t cÃ¡c IDs.
    **Do Ä‘Ã³, khi huáº¥n luyá»‡n trÃªn táº­p dá»¯ liá»‡u "nÃ´ng" nhÆ° váº­y, mÃ´ hÃ¬nh sáº½ cÃ³ xu hÆ°á»›ng thoÃ¡i hÃ³a (degeneration) vÃ  quÃ¡ khá»›p (overfitting) do thiáº¿u dá»¯ liá»‡u Ä‘a dáº¡ng cho tá»«ng lá»›p.**

    áº¢nh hÆ°á»Ÿng cá»§a viá»‡c sá»­ dá»¥ng Softmax Loss vÃ  cÃ¡c hÃ m máº¥t mÃ¡t cÃ³ margin gÃ³c:
        CÃ¡c hÃ m máº¥t mÃ¡t nhÆ° Softmax Loss vÃ  nhá»¯ng biáº¿n thá»ƒ cÃ³ margin gÃ³c (nhÆ° CosFace) thÆ°á»ng khÃ´ng Ä‘á»§ kháº£ nÄƒng xá»­ lÃ½ tá»‘t cÃ¡c táº­p dá»¯ liá»‡u "nÃ´ng".
        TrÃªn cÃ¡c táº­p dá»¯ liá»‡u shallow, cÃ¡c hÃ m máº¥t mÃ¡t nÃ y dá»… gÃ¢y ra thoÃ¡i hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng (feature space collapse), khiáº¿n cÃ¡c vector Ä‘áº·c trÆ°ng khÃ´ng Ä‘Æ°á»£c phÃ¢n biá»‡t tá»‘t giá»¯a cÃ¡c ID, tá»« Ä‘Ã³ lÃ m giáº£m hiá»‡u quáº£ nháº­n diá»‡n.
    Feature Space Collapse:
        Feature space collapse lÃ  hiá»‡n tÆ°á»£ng khi cÃ¡c vector Ä‘áº·c trÆ°ng cá»§a cÃ¡c ID trong khÃ´ng gian Ä‘áº·c trÆ°ng trá»Ÿ nÃªn gáº§n nhau, khiáº¿n chÃºng khÃ´ng Ä‘á»§ kháº£ nÄƒng phÃ¢n biá»‡t giá»¯a cÃ¡c danh tÃ­nh khÃ¡c nhau.
        Khi dá»¯ liá»‡u khÃ´ng Ä‘á»§ phong phÃº, khÃ´ng gian Ä‘áº·c trÆ°ng bá»‹ thu háº¹p láº¡i (collapse), vÃ  mÃ´ hÃ¬nh sáº½ khÃ´ng thá»ƒ táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng Ä‘a dáº¡ng cáº§n thiáº¿t Ä‘á»ƒ phÃ¢n biá»‡t cÃ¡c ID khÃ¡c nhau. Äiá»u nÃ y dáº«n Ä‘áº¿n viá»‡c mÃ´ hÃ¬nh khÃ´ng há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c Ä‘iá»ƒm phÃ¢n biá»‡t quan trá»ng giá»¯a cÃ¡c danh tÃ­nh.

    Li et al. [76] Ä‘á» xuáº¥t khÃ¡i niá»‡m virtual class, giáº£i phÃ¡p nÃ y dÃ¹ng Ä‘á»ƒ xá»­ lÃ½ cÃ¡c áº£nh khÃ´ng cÃ³ label. Thay vÃ¬ yÃªu cáº§u dá»¯ liá»‡u cÃ³ nhÃ£n Ä‘áº§y Ä‘á»§ hoáº·c sá»‘ lÆ°á»£ng máº«u lá»›n cho má»—i ID, phÆ°Æ¡ng phÃ¡p nÃ y sá»­ dá»¥ng dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n vÃ  gÃ¡n cho má»—i áº£nh má»™t lá»›p áº£o (virtual class) Ä‘á»ƒ Ä‘áº¡i diá»‡n cho danh tÃ­nh giáº£ Ä‘á»‹nh cá»§a nÃ³.
        Trong má»—i mini-batch, má»—i áº£nh khÃ´ng cÃ³ nhÃ£n Ä‘Æ°á»£c xem lÃ  trung tÃ¢m cá»§a má»™t lá»›p áº£o (virtual class) vÃ  Ä‘Æ°á»£c coi nhÆ° má»™t lá»›p Ã¢m (negative class).
        CÃ´ng thá»©c máº¥t mÃ¡t (loss) Ä‘Æ°á»£c má»Ÿ rá»™ng báº±ng cÃ¡ch thÃªm cÃ¡c lá»›p áº£o vÃ o cÃ´ng thá»©c cá»§a hÃ m máº¥t mÃ¡t dá»±a trÃªn margin lá»›n (vÃ­ dá»¥ nhÆ° ArcFace):
            ![](images/4.1.3.%20Virtual%20class.png)

        TÄƒng cÆ°á»ng Ä‘áº·c trÆ°ng: Äá»ƒ khai thÃ¡c tá»‘t hÆ¡n dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n, má»™t bá»™ sinh Ä‘áº·c trÆ°ng (feature generator) Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng tÄƒng cÆ°á»ng tá»« dá»¯ liá»‡u khÃ´ng cÃ³ nhÃ£n, tá»« Ä‘Ã³ giÃºp tÄƒng tÃ­nh phÃ¢n biá»‡t.

    ![](images/4.1.3.%20Meta-learning.png)

### Tá»•ng káº¿t

Báº£n cháº¥t 1 mÃ´ hÃ¬nh FR lÃ  thuá»™c vá» bÃ i toÃ¡n phÃ¢n loáº¡i (classification). Äáº§u tiÃªn model sáº½ extract cÃ¡c feature cá»§a face image. Sau Ä‘Ã³ cá»‘ gáº¯ng tÃ¬m 1 space (hyperspace) nÃ o Ä‘Ã³ Ä‘á»ƒ biá»ƒu diá»…n cÃ¡c feature nÃ y, sao cho cÃ¡c feature cá»§a cÃ¹ng 1 face ID sáº½ á»Ÿ gáº§n nhau vÃ  cÃ¡c feature cá»§a cÃ¡c image khÃ¡c face ID sáº½ xa, khÃ¡c nhau.

Hyperspace

![](images/4.1.%20hyperspac.png)

![](images/4.1.%20hypershere.png)

Khoáº£ng cÃ¡ch euclidean vÃ  gÃ³c.

![](images/4.1.%20Euclidean.png)

![](images/4.1.%20GÃ³c.png)

![](images/4.1.%20Example.png)

Má»—i áº£nh sau khi Ä‘Æ°á»£c model trÃ­ch xuáº¥t sáº½ cho ra 1 feature vector tÆ°Æ¡ng á»©ng vá»›i nhiá»u trÆ°á»ng chá»© khÃ´ng pháº£i lÃ  1 Ä‘iá»ƒm (lÃ½ do lÃ  vÃ¬ khuÃ´n máº·t phá»©c táº¡p nÃªn khÃ´ng tá»“n táº¡i 1 Ä‘iá»ƒm thá»a mÃ£n). CÃ¡c Ä‘áº·c trÆ°ng vá» hÃ¬nh dáº¡ng, Ä‘á»™ sÃ¡ng vÃ  cÃ¡c chi tiáº¿t khuÃ´n máº·t khÃ¡c Ä‘Æ°á»£c giá»¯ láº¡i vÃ  thá»ƒ hiá»‡n qua cÃ¡c giÃ¡ trá»‹ trong vector Ä‘Ã³. Do Ä‘Ã³ ta cÃ³ thá»ƒ dÃ¹ng khoáº£ng cÃ¡ch euclidean hoáº·c cÃ´ng thá»©c cosin Ä‘á»ƒ tÃ­nh toÃ¡n khoáº£ng cÃ¡ch/gÃ³c giá»¯a 2 vector nÃ y. Trong khÃ´ng gian Ä‘áº·c trÆ°ng, viá»‡c so sÃ¡nh giá»¯a cÃ¡c vector trá»Ÿ nÃªn há»¯u Ã­ch, vÃ¬ chÃºng ta cÃ³ thá»ƒ dÃ¹ng khoáº£ng cÃ¡ch Euclidean hoáº·c gÃ³c cosine giá»¯a cÃ¡c vector Ä‘á»ƒ Ä‘o Ä‘á»™ giá»‘ng nhau. Hai vector gáº§n nhau hoáº·c cÃ³ gÃ³c nhá» giá»¯a chÃºng sáº½ Ä‘áº¡i diá»‡n cho hai áº£nh cÃ³ cÃ¡c Ä‘áº·c trÆ°ng tÆ°Æ¡ng tá»±.

CÃ¡c phÆ°Æ¡ng phÃ¡p á»Ÿ pháº§n 4.1.1 cho ra hiá»‡u xuáº¥t tháº¥p hÆ¡n so vá»›i cÃ¡c phÆ°Æ¡ng  phÃ¡p á»Ÿ pháº§n 4.1.2. VÃ  phÆ°Æ¡ng phÃ¡p máº¡nh nháº¥t trong bÃ i reseach nÃ y lÃ  MagFace vÃ  AdaFace (xem á»Ÿ pháº§n 6)

á» má»¥c 4.1.1, ta tÃ¬m hiá»ƒu cÃ¡c loss function cÃ³ vai trÃ² nhÆ° 1 metric trong quÃ¡ trÃ¬nh train. Má»¥c tiÃªu cá»§a quÃ¡ trÃ¬nh train lÃ  tá»‘i thiá»ƒu hÃ³a giÃ¡ trá»‹ loss nÃ y vÃ  báº£n thÃ¢n Loss chÃ­nh lÃ  khoáº£ng cÃ¡ch euclidean hay cosin giá»¯a cÃ¡c feature vector extract tá»« face image.
1. **cross-entropy**: Äá»‘i vá»›i lÄ©nh vá»±c face verification, ta cÃ³ thá»ƒ sá»­ dá»¥ng crosss-entropy lÃ m loss function.
2. **contrastive loss**: *Ká»ƒ tá»« loss nÃ y báº¯t Ä‘áº§u xuáº¥t hiá»‡n positive pair/ negative pair, margin m, khoáº£ng cÃ¡ch euclidean/cosin*
    Má»¥c tiÃªu cá»§a hÃ m loss nÃ y lÃ  sao cho khoáº£ng cÃ¡ch giá»¯a 2 feature thuá»™c cÃ¹ng 1 face ID sáº½ cÃ ng gáº§n nhau vÃ  khoáº£ng cÃ¡ch giá»¯a 2 feature khÃ¡c face ID sáº½ cÃ¡ch xa nhau. Tá»©c lÃ  hÃ m loss nÃ y sáº½ thá»±c hiá»‡n so sÃ¡nh tá»«ng cáº·p image 1 trong dataset => Há»™i tá»¥ ráº¥t lÃ¢u.
    HÃ m nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n theo khoáº£ng cÃ¡ch euclidean hoáº·c cosin

    ![](images/4.1.1.%20contrastive.png)
    
    ![](images/4.1.1.%20contrastive-cosin.png)

    **Trong Ä‘Ã³ margin m lÃ  tham sá»‘ cá»‘ Ä‘á»‹nh pháº£i truyá»n vÃ o trong quÃ¡ trÃ¬nh train.**
3. **BioMetricNet**
    Hiá»ƒu nÃ´m la lÃ  nÃ³ ráº¥t phá»©c táº¡p, má»¥c tiÃªu cá»§a hÃ m nÃ y khÃ´ng liÃªn quan gÃ¬ Ä‘áº¿n khoáº£ng cÃ¡ch mÃ  lÃ  cá»‘ map cÃ¡c feature vector vÃ o 1 space sao cho positive pair vÃ  negative pair sáº½ thuá»™c vÃ o 1 dáº¡ng phÃ¢n phá»‘i nÃ o Ä‘Ã³ => KhÃ´ng quan tÃ¢m vÃ¬ khÃ´ng dÃ¹ng
4. **Triplet loss**
    Äáº§u vÃ o cá»§a nÃ³ gá»“m 3 áº£nh anchor, positive (cÃ¹ng ID vá»›i anchor) vÃ  negative (khÃ¡c ID vá»›i anchor).
    Bá»‘i cáº£nh ra Ä‘á»i cá»§a hÃ m nÃ y lÃ  quÃ¡ trÃ¬nh phÃ¢n cá»¥m cÃ¡c áº£nh thuá»™c cÃ¹ng 1 ID pháº£i á»Ÿ 1 chá»— riÃªng vá»›i chá»— cÃ¡c cÃ¡c face ID khÃ¡c.
    **Má»¥c tiÃªu cá»§a hÃ m nÃ y lÃ  tá»‘i Ä‘a khoáº£ng cÃ¡ch (x^a,x^n) vÃ  tá»‘i thiá»ƒu hÃ³a khoáº£ng cÃ¡ch (x^a, x^p) vÃ  (x^a,x^n)-(x^a,x^p) > alpha (hay lÃ  margin)**. Trong Ä‘Ã³ x lÃ  feature cá»§a face Ä‘Æ°á»£c model extract tá»« image, x^a lÃ  anchor feature, x^p lÃ  postive feature, x^n lÃ  negative feature. Hiá»ƒn nhiÃªn quÃ¡ trÃ¬nh nÃ y láº·p láº¡i vá»›i bá»™ 3 áº£nh Ä‘Æ°á»£c láº¥y tá»« dataset => Há»™i tá»¥ cá»±c lÃ¢u.

    Äá»ƒ quÃ¡ trÃ¬nh train diá»…n ra nhanh hÆ¡n => Há»™i tá»¥ pháº£i nhanh => Ta pháº£i chá»n ra cÃ¡c bá»™ 3 áº£nh tá»‡ nháº¥t cÃ³ thá»ƒ xáº£y ra vÃ  lÃ m cho nÃ³ thá»a mÃ£n Ä‘iá»u kiÃªn train thÃ¬ cÃ¡c bá»™ 3 khÃ¡c cháº¯c cháº¯n thá»a mÃ£n => KhÃ¡i niá»‡m hard positive vÃ  hard negative ra Ä‘á»i.
        hard-positive: khoáº£ng cÃ¡ch (xp,xa) lá»›n nháº¥t => GiÃºp mÃ´ hÃ¬nh há»c cÃ¡ch thu nhá» khoáº£ng cÃ¡ch giá»¯a anchor vÃ  positive.
        hard-negative: khoáº£ng cÃ¡ch (xn,xa) nhá» nháº¥t => Nháº±m tÄƒng Ä‘á»™ khÃ³ cho mÃ´ hÃ¬nh khi cá»‘ gáº¯ng phÃ¢n biá»‡t anchor vá»›i negative.
5. **Kang loss**
    Kang vÃ  cÃ¡c cá»™ng sá»± cá»§a anh Ä‘Ã£ Ä‘Æ¡n giáº£n hÃ³a contrastive loss vÃ  triplet loss vÃ o 1 cÃ´ng thá»©c.
    ...
6. **center loss**
    PhÃ¡t triá»ƒn thÃªm cÃ¡c hÃ m trÃªn báº±ng cÃ¡ch giáº£m thiá»‡u sá»± phÃ¢n tÃ¡n cá»§a cÃ¡c feature trong cÃ¹ng 1 class, Ä‘iá»u mÃ  cÃ¡c loss function trÃªn chÆ°a lÃ m Ä‘Æ°á»£c. Cá»¥ thá»ƒ, nÃ³ sáº½ tÃ¬m 1 train space sao cho cÃ¡c feature thuá»™c cÃ¹ng 1 face ID sáº½ phÃ¢n tÃ¡n gáº§n vá» phÃ­a trung tÃ¢m cá»§a class Ä‘Ã³ hÆ¡n.

    ![](images/4.1.1.%20center.png)

á» má»¥c 4.1.2, ta báº¯t Ä‘áº§u tÃ¬m hiá»ƒu 1 loáº¡i loss má»›i dá»±a trÃªn margin. Margin lÃ  giÃ¡ trá»‹ biÃªn Ä‘á»™, Ä‘áº£m báº£o khoáº£ng cÃ¡ch giá»¯a 2 cÃ¡i gÃ¬ Ä‘Ã³ luÃ´n lá»›n hÆ¡n margin. ÄÃ¢y lÃ  dáº¡ng loss function cÃ³ cÃ´ng thá»©c chá»© khÃ´ng pháº£i lÃ  1 metric learning nhÆ° cÃ¡c loss á»Ÿ trÃªn.
1. Softmax Loss cÆ¡ báº£n
    Ã tÆ°á»Ÿng: Softmax loss lÃ  hÃ m máº¥t mÃ¡t cÆ¡ báº£n cho cÃ¡c bÃ i toÃ¡n phÃ¢n loáº¡i, nhÆ°ng khÃ´ng tá»‘i Æ°u cho viá»‡c nháº­n diá»‡n khuÃ´n máº·t. Softmax dá»±a trÃªn viá»‡c tÃ­nh toÃ¡n khoáº£ng cÃ¡ch giá»¯a feature vector cá»§a áº£nh vÃ  trá»ng sá»‘ cá»§a lá»›p Ä‘á»ƒ phÃ¢n loáº¡i, nhÆ°ng khÃ´ng khuyáº¿n khÃ­ch sá»± phÃ¢n biá»‡t rÃµ rÃ ng giá»¯a cÃ¡c lá»›p.

    Giá»›i háº¡n: Thiáº¿u kháº£ nÄƒng tÄƒng cÆ°á»ng khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p khÃ¡c nhau, dáº«n Ä‘áº¿n khÃ´ng gian Ä‘áº·c trÆ°ng kÃ©m phÃ¢n biá»‡t (khÃ´ng cÃ³ kháº£ nÄƒng Ä‘áº©y cÃ¡c feature vector cá»§a cÃ¡c class khÃ¡c ra xa vÃ  lÃ m feature cá»§a cÃ¹ng class gáº§n nhau).

2. L-Softmax vÃ  A-Softmax (SphereFace)
    Ã tÆ°á»Ÿng: Bá»• sung margin gÃ³c vÃ o softmax, thay Ä‘á»•i khoáº£ng cÃ¡ch giá»¯a feature vector cá»§a áº£nh vÃ  trá»ng sá»‘ lá»›p báº±ng cÃ¡ch thÃªm má»™t giÃ¡ trá»‹ margin vÃ o gÃ³c giá»¯a chÃºng.

    Cáº£i tiáº¿n so vá»›i Softmax: Khuyáº¿n khÃ­ch cÃ¡c vector cá»§a cÃ¹ng má»™t lá»›p náº±m gáº§n nhau hÆ¡n vÃ  cÃ¡ch xa cÃ¡c lá»›p khÃ¡c. A-softmax Ä‘Æ°a thÃªm margin vÃ o gÃ³c giá»¯a cÃ¡c Ä‘áº·c trÆ°ng cá»§a positive class vÃ  negative class, giÃºp cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t dá»… phÃ¢n biá»‡t hÆ¡n.

    Giá»›i háº¡n: 
        Phá»©c táº¡p trong huáº¥n luyá»‡n, Ä‘áº·c biá»‡t vá»›i dá»¯ liá»‡u nhiá»…u hoáº·c máº¥t cÃ¢n báº±ng.
        Huáº¥n luyá»‡n phá»©c táº¡p vÃ  yÃªu cáº§u Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ nháº¡y cáº£m.

3. NormFace
    Ã tÆ°á»Ÿng: Chuáº©n hÃ³a embedding vÃ  trá»ng sá»‘ lá»›p, sau Ä‘Ã³ má»Ÿ rá»™ng embedding lÃªn má»™t tham sá»‘ ğ‘  Ä‘á»ƒ Ä‘iá»u chá»‰nh sá»± phÃ¢n bá»‘ cÃ¡c Ä‘áº·c trÆ°ng trÃªn má»™t hypersphere.

    Cáº£i tiáº¿n: TÄƒng Ä‘á»™ á»•n Ä‘á»‹nh vÃ  kháº£ nÄƒng phÃ¢n biá»‡t Ä‘áº·c trÆ°ng, giáº£m bá»›t sá»± phá»¥ thuá»™c vÃ o cÃ¡c margin phá»©c táº¡p. TÄƒng cÆ°á»ng tÃ­nh á»•n Ä‘á»‹nh, nhÆ°ng khÃ´ng sá»­ dá»¥ng margin cho cÃ¡c lá»›p dÆ°Æ¡ng nhÆ° SphereFace.

4. CosFace vÃ  ArcFace
    Ã tÆ°á»Ÿng: ThÃªm margin gÃ³c vÃ o softmax Ä‘á»ƒ táº¡o khoáº£ng cÃ¡ch giá»¯a cÃ¡c lá»›p.
        CosFace trá»« margin vÃ o cosine cá»§a gÃ³c giá»¯a Ä‘áº·c trÆ°ng vÃ  trá»ng sá»‘ lá»›p.
        ArcFace Ä‘Æ°a margin vÃ o trong hÃ m cosine, táº¡o margin dáº¡ng cung (angular margin) Ä‘á»ƒ cáº£i thiá»‡n tÃ­nh phÃ¢n biá»‡t.
    Cáº£i tiáº¿n: CosFace vÃ  ArcFace tÄƒng Ä‘á»™ chÃ­nh xÃ¡c nháº­n diá»‡n, tá»‘i Æ°u hÃ³a phÃ¢n tÃ¡ch giá»¯a cÃ¡c lá»›p báº±ng cÃ¡ch trá»±c tiáº¿p thao tÃ¡c trÃªn gÃ³c.

    Giá»›i háº¡n: ÄÃ²i há»i Ä‘iá»u chá»‰nh cÃ¡c siÃªu tham sá»‘ nháº¡y cáº£m, dá»… lÃ m máº¥t á»•n Ä‘á»‹nh khi huáº¥n luyá»‡n.

5. SV-Softmax
    Ã tÆ°á»Ÿng: SV-Softmax chá»n cÃ¡c máº«u Ã¢m khÃ³ (hard negative) vÃ  Ä‘áº©y nÃ³ ra xa trung tÃ¢m cÃ¡c lá»›p positive, giÃºp cáº£i thiá»‡n sá»± phÃ¢n tÃ¡ch ná»™i lá»›p.

    Cáº£i tiáº¿n: ÄÆ°a ra chiáº¿n lÆ°á»£c hard negative mining, Ä‘áº©y máº¡nh sá»± gáº¯n káº¿t cá»§a cÃ¡c máº«u trong lá»›p dÆ°Æ¡ng báº±ng cÃ¡ch xá»­ lÃ½ cÃ¡c máº«u Ã¢m khÃ³.

6. Ring Loss
    Ã tÆ°á»Ÿng: Giá»¯ Ä‘á»™ dÃ i cá»§a cÃ¡c embedding khÃ´ng Ä‘á»•i á»Ÿ giÃ¡ trá»‹ ğ‘…, giÃºp embedding cÃ³ cÃ¹ng Ä‘á»™ lá»›n vÃ  tÄƒng cÆ°á»ng tÃ­nh á»•n Ä‘á»‹nh.

    Cáº£i tiáº¿n: Há»— trá»£ cÃ¡c hÃ m máº¥t mÃ¡t khÃ¡c duy trÃ¬ Ä‘á»™ dÃ i embedding, giáº£m thiá»ƒu áº£nh hÆ°á»Ÿng cá»§a nhiá»…u trong khÃ´ng gian Ä‘áº·c trÆ°ng.

7. MagFace
    Ã tÆ°á»Ÿng: Äiá»u chá»‰nh margin theo Ä‘á»™ lá»›n cá»§a Ä‘áº·c trÆ°ng khuÃ´n máº·t theo cháº¥t lÆ°á»£ng máº«u. Máº«u dá»… sáº½ cÃ³ Ä‘á»™ lá»›n cao vÃ  náº±m gáº§n trung tÃ¢m, máº«u khÃ³ hoáº·c nhiá»…u cÃ³ Ä‘á»™ lá»›n nhá» vÃ  cÃ¡ch xa.
    
    Cáº£i tiáº¿n: Káº¿t há»£p cáº£ margin vÃ  Ä‘á»™ lá»›n, giÃºp tá»‘i Æ°u hÃ³a phÃ¢n bá»‘ cá»§a cÃ¡c máº«u dá»… vÃ  khÃ³ trong khÃ´ng gian Ä‘áº·c trÆ°ng, Ä‘á»“ng thá»i tÄƒng tÃ­nh chá»‘ng nhiá»…u.

8. AdaFace
    Ã tÆ°á»Ÿng: Äiá»u chá»‰nh gradient cá»§a cÃ¡c máº«u khÃ³ dá»±a trÃªn cháº¥t lÆ°á»£ng cá»§a áº£nh. Khi cháº¥t lÆ°á»£ng cao, máº«u khÃ³ Ä‘Æ°á»£c nháº¥n máº¡nh; khi cháº¥t lÆ°á»£ng tháº¥p, má»©c Ä‘á»™ áº£nh hÆ°á»Ÿng cá»§a máº«u khÃ³ giáº£m.

    Cáº£i tiáº¿n: Tá»± Ä‘á»™ng Ä‘iá»u chá»‰nh margin theo cháº¥t lÆ°á»£ng dá»¯ liá»‡u, cáº£i thiá»‡n tÃ­nh á»•n Ä‘á»‹nh cá»§a mÃ´ hÃ¬nh khi gáº·p dá»¯ liá»‡u Ä‘a dáº¡ng vá» cháº¥t lÆ°á»£ng.

9. Sub-center ArcFace
    Ã tÆ°á»Ÿng: Chia cÃ¡c máº«u cá»§a má»™t danh tÃ­nh thÃ nh nhiá»u sub-center, vá»›i má»™t sub-center chá»©a cÃ¡c máº«u sáº¡ch vÃ  cÃ¡c sub-center cÃ²n láº¡i chá»©a máº«u khÃ³ hoáº·c nhiá»…u.

    Cáº£i tiáº¿n: Giáº£m Ã¡p lá»±c rÃ ng buá»™c ná»™i lá»›p, cáº£i thiá»‡n kháº£ nÄƒng phÃ¢n loáº¡i khi cÃ³ dá»¯ liá»‡u nhiá»…u báº±ng cÃ¡ch xá»­ lÃ½ dá»¯ liá»‡u theo tá»«ng sub-class.

10. CurricularFace
    Ã tÆ°á»Ÿng: Ãp dá»¥ng curriculum learning Ä‘á»ƒ há»c tá»« cÃ¡c máº«u dá»… á»Ÿ giai Ä‘oáº¡n Ä‘áº§u vÃ  cÃ¡c máº«u khÃ³ á»Ÿ giai Ä‘oáº¡n sau cá»§a quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

    Cáº£i tiáº¿n: Cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c vÃ  tÃ­nh phÃ¢n biá»‡t báº±ng cÃ¡ch táº­p trung vÃ o máº«u dá»… trÆ°á»›c khi chuyá»ƒn sang máº«u khÃ³, cáº­p nháº­t trá»ng sá»‘ Ä‘á»™ng qua Exponential Moving Average (EMA) Ä‘á»ƒ tá»‘i Æ°u hÃ³a quÃ¡ trÃ¬nh huáº¥n luyá»‡n.

11. NPCface
    Ã tÆ°á»Ÿng: Nháº¥n máº¡nh cÃ¡c máº«u khÃ³ cáº£ vá» dÆ°Æ¡ng vÃ  Ã¢m thÃ´ng qua collaborative margin Ä‘á»ƒ xá»­ lÃ½ cÃ¡c táº­p dá»¯ liá»‡u lá»›n, nÆ¡i cÃ¡c máº«u hard positive vÃ  hard negative thÆ°á»ng xuáº¥t hiá»‡n cÃ¹ng nhau.

    Cáº£i tiáº¿n: TÄƒng kháº£ nÄƒng phÃ¢n biá»‡t vá»›i cÃ¡c táº­p dá»¯ liá»‡u lá»›n, giÃºp mÃ´ hÃ¬nh táº­p trung vÃ o cÃ¡c máº«u khÃ³ má»™t cÃ¡ch toÃ n diá»‡n hÆ¡n.

12. UniformFace
    Ã tÆ°á»Ÿng: PhÃ¢n bá»‘ Ä‘á»“ng Ä‘á»u cÃ¡c lá»›p trong khÃ´ng gian Ä‘áº·c trÆ°ng trÃªn má»™t hypersphere, tá»‘i Æ°u hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng báº±ng cÃ¡ch giá»¯ khoáº£ng cÃ¡ch tá»‘i thiá»ƒu giá»¯a cÃ¡c lá»›p.
    
    Cáº£i tiáº¿n: Tá»‘i Ä‘a hÃ³a kháº£ nÄƒng khai thÃ¡c khÃ´ng gian Ä‘áº·c trÆ°ng, giáº£m thiá»ƒu hiá»‡n tÆ°á»£ng chá»“ng chÃ©o giá»¯a cÃ¡c lá»›p vÃ  tÄƒng cÆ°á»ng kháº£ nÄƒng phÃ¢n biá»‡t.

## 4.2. Embedding
KhÃ¡c vá»›i viá»‡c thiáº¿t káº¿ cÃ¡c loss function á»Ÿ trÃªn, embedding refinement (tinh chá»‰nh embedding) lÃ  1 cÃ¡ch khÃ¡c Ä‘á»ƒ tÄƒng cÆ°á»ng káº¿t quáº£ FR.
- Ã tÆ°á»Ÿng 1: thiáº¿t láº­p cÃ¡c rÃ ng buá»™c rÃµ rÃ ng giá»¯a face embeddings vá»›i face generators (trÃ¬nh táº¡o khuÃ´n máº·t), giÃºp cáº£i thiá»‡n kháº£ nÄƒng phÃ¢n biá»‡t cá»§a cÃ¡c vector Ä‘áº·c trÆ°ng trong khÃ´ng gian Ä‘áº·c trÆ°ng.
- Ã tÆ°á»Ÿng 2: thay Ä‘á»•i face embeddings báº±ng cÃ¡c thÃ´ng tin phá»¥ trá»£ láº¥y Ä‘Æ°á»£c tá»« training image nhÆ° Ä‘á»™ che khuáº¥t (occlusion), Ä‘á»™ phÃ¢n giáº£i há»‰nh áº£nh (resolution) nháº±m cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c nháº­n diá»‡n ngay cáº£ khi cÃ³ cÃ¡c yáº¿u tá»‘ nhiá»…u.
- Ã tÆ°á»Ÿng 3: models FR in multi-tasks. CÃ¡c tasks nhÆ° Ä‘oÃ¡n tuá»•i vÃ  tÆ° tháº¿ cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c thÃªm vÃ o máº¡ng, giÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng bá»• sung cÃ³ lá»£i cho viá»‡c phÃ¢n biá»‡t khuÃ´n máº·t.

### 4.2.1. Embedding refinement by face generator (Tinh chá»‰nh Face Embedding báº±ng face generator)

1. **DR-GAN[78] - Deep Reconstruction Generative Adversarial Network**
    **CÃ¡c giáº£i phÃ¡p FR dá»±a trÃªn face generator thÆ°á»ng dá»±a trÃªn yáº¿u tá»‘ face sáº½ báº¥t biáº¿n theo tuá»•i hoáº·c tÆ° tháº¿ (gÃ³c chá»¥p). => Invariance (tÃ­nh báº¥t biáº¿n)**
    DR-GAN[78]: Ä‘Ã£ giáº£i quáº¿t váº¥n Ä‘á» face báº¥t biáº¿n theo tÆ° tháº¿ báº±ng cÃ¡ch tá»•ng há»£p cÃ¡c khuÃ´n máº·t cÃ³ tÆ° tháº¿ khÃ¡c nhau. Cá»¥ thá»ƒ **máº¡ng nÃ y sáº½ há»c cÃ¡ch biá»ƒu diá»…n face image bá»Ÿi 1 kiáº¿n trÃºc encoder-decoder generator**. Äáº§u ra cá»§a decoder tá»•ng há»£ nhiá»u khuÃ´n  máº·t cá»§a cÃ¹ng 1 ID vá»›i cÃ¡c pose (tÆ° tháº¿) khÃ¡c nhau.
        encoder (Genc): Ä‘Æ°a vÃ o ID label y^d vÃ  label tÆ° tháº¿ y^p cá»§a face image x. Encoder sáº½ extract tÆ° tháº¿ trong áº£nh táº¡o ra emdedding dá»±a theo tÆ° tháº¿ gá»‘c cá»§a áº£nh f(x) = Genc(x). Sau Ä‘Ã³ f(x) Ä‘Æ°á»£c concate vá»›i pose code c (mÃ£ tÆ° tháº¿) vÃ  1 chá»‰ sá»‘ random noise z.
        decoder (Gdec): táº¡o ra cÃ¡c face image vá»›i cÃ¡c tÆ° tháº¿ khÃ¡c nhau báº±ng cÃ¡ch giáº£i mÃ£ embedding Gdec = (f(x), c, z) vÃ  hiá»ƒn nhiÃªn label váº«n tháº¿.

    Vá»›i má»—i áº£nh Ä‘Æ°á»£c tá»•ng há»£p (synthetic) Ä‘Æ°á»£c táº¡o ra bá»Ÿi phÆ°Æ¡ng phÃ¡p trÃªn (bá»™ generator), disciminator D (bá»™ phÃ¢n biá»‡t D) sáº½ cá»‘ gáº¯ng Æ°á»›c tÃ­nh xem áº£nh má»›i x~ cÃ¹ng tÆ° tháº¿ cá»§a nÃ³ cÃ³ pháº£i lÃ  áº£nh fake hay khÃ´ng.

    **TÃ³m láº¡i lÃ  há»c Ä‘á»‘i khÃ¡ng**

2. **D2AE - Identity Distilling and Dispelling Auto-encoder** (chÆ°ng cáº¥t vÃ  phÃ¢n tÃ¡n)
    **Liu et al. [79] Ä‘á» xuáº¥t sá»­ dá»¥ng má»™t máº¡ng auto-encoder** Ä‘á»ƒ há»c cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t phá»¥c vá»¥ cho xÃ¡c minh danh tÃ­nh vÃ  nháº±m cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c báº±ng cÃ¡ch táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng tÃ¡ch biá»‡t rÃµ rÃ ng cho danh tÃ­nh (identity-distilled) vÃ  cÃ¡c yáº¿u tá»‘ khÃ¡c ngoÃ i danh tÃ­nh (identity-dispelled).

    Cá»¥ thá»ƒ encoder nháº­n vÃ o 1 áº£nh x vÃ  cá»‘ gáº¯ng extract feature cá»§a áº£nh. Sau Ä‘Ã³ nÃ³ cá»‘ gáº¯ng tÃ¡ch feature nÃ y thanh 2 nhÃ¡nh. 
        Distilling Bt: NhÃ¡nh chá»‰ chá»©a cÃ¡c feature phá»¥c vá»¥ xÃ¡c minh danh tÃ­nh vÃ  Ä‘Æ°á»£c tá»‘i Æ°u hÃ³a bá»Ÿi hÃ m softmax
        Dispelling Bp: NhÃ¡nh cá»‘ gáº¯ng loáº¡i bá» cÃ¡c feature liÃªn quan Ä‘áº¿n danh tÃ­nh trong feature gá»‘c cá»§a áº£nh. Káº¿t quáº£ lÃ  1 feauture chá»©a cÃ¡c yáº¿u tá»‘ nhÆ° Ã¡nh sÃ¡ng, gÃ³c chá»¥p, ... . NhÃ¡nh nÃ y cá»‘ gáº¯ng Ä‘Ã¡nh lá»«a decoder báº±ng cÃ¡ch táº¡o ra 1 phÃ¢n phá»‘i danh tÃ­nh cá»‘ Ä‘á»‹nh.

    Decoder nháº­n cáº£ 2 vector fT vÃ  fP á»Ÿ trÃªn lÃ m Ä‘áº§u vÃ o vÃ  Ä‘áº£m báº£o 2 cÃ¡i nÃ y Ä‘Æ°á»£c báº£o toÃ n trong khÃ´ng gian Ä‘áº·c trÆ°ng

    ![](images/4.2.1.%20D2AE%20-%20arichitech.png)

    ![](images/4.2.1.%20D2AE.png)

3. **R3AN**
    Chen et al. [80] Ä‘á» xuáº¥t Ä‘á»ƒ giáº£i quyáº¿t bÃ i toÃ¡n Cross Model Face Recognition (CMFR) báº±ng máº¡ng R3AN.
    
    CMFR lÃ  bÃ i toÃ¡n nháº­n diá»‡n khuÃ´n máº·t báº±ng cÃ¡ch nháº­n dáº¡ng Ä‘áº·c trÆ°ng Ä‘áº§u vÃ o tá»« má»™t mÃ´ hÃ¬nh báº±ng cÃ¡ch sá»­ dá»¥ng dá»¯ liá»‡u tá»« má»™t mÃ´ hÃ¬nh khÃ¡c. 
    
    PhÆ°Æ¡ng phÃ¡p R3AN gá»“m ba thÃ nh pháº§n chÃ­nh: reconstruction (tÃ¡i táº¡o), representation (Ä‘áº¡i diá»‡n Ä‘áº·c trÆ°ng), vÃ  regression (há»“i quy).

    ![](images/4.2.1.%20R3AN.png)

4. **MTLFace**
    Huang et al. [81] Ä‘á» xuáº¥t, má»™t phÆ°Æ¡ng phÃ¡p há»c sÃ¢u káº¿t há»£p Ä‘á»ƒ xá»­ lÃ½ Ä‘á»“ng thá»i bÃ i toÃ¡n nháº­n diá»‡n khuÃ´n máº·t khÃ´ng phá»¥ thuá»™c tuá»•i (age-invariant face recognition, AIFR) vÃ  dá»± Ä‘oÃ¡n tuá»•i cá»§a khuÃ´n máº·t (face age synthesis, FAS)

    MTLFace sá»­ dá»¥ng má»™t kiáº¿n trÃºc encoder-decoder, cÃ¹ng vá»›i cÆ¡ cháº¿ attention vÃ  adversarial learning Ä‘á»ƒ táº¡o ra embedding khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi tuá»•i tÃ¡c vÃ  cÃ³ kháº£ nÄƒng tá»•ng há»£p khuÃ´n máº·t á»Ÿ cÃ¡c Ä‘á»™ tuá»•i khÃ¡c nhau.

    ![](images/4.2.1.%20MTLFace.png)

5. **TS-GAN -Teacher-Student Generative Adversarial Network ***
    Táº¡o ra hÃ¬nh áº£nh Ä‘á»™ sÃ¢u tá»« hÃ¬nh áº£nh RGB Ä‘Æ¡n (mÃ u) nháº±m cáº£i thiá»‡n hiá»‡u suáº¥t cá»§a há»‡ thá»‘ng nháº­n diá»‡n khuÃ´n máº·t.
    Cáº¥u trÃºc: Bao gá»“m hai thÃ nh pháº§n chÃ­nh lÃ  giÃ¡o viÃªn (teacher) vÃ  há»c sinh (student).
        GiÃ¡o viÃªn: Gá»“m má»™t generator (táº¡o hÃ¬nh) vÃ  má»™t discriminator (phÃ¢n loáº¡i) há»c cÃ¡ch Ã¡nh xáº¡ giá»¯a kÃªnh RGB vÃ  Ä‘á»™ sÃ¢u tá»« hÃ¬nh áº£nh RGB-D (hÃ¬nh áº£nh cÃ³ Ä‘á»™ sÃ¢u).
        Há»c sinh: Cáº£i thiá»‡n Ã¡nh xáº¡ Ä‘Ã£ há»c cho hÃ¬nh áº£nh RGB báº±ng cÃ¡ch huáº¥n luyá»‡n thÃªm generator.
    QuÃ¡ trÃ¬nh huáº¥n luyá»‡n: MÃ´ hÃ¬nh nháº­n vÃ o hÃ¬nh áº£nh RGB vÃ  táº¡o ra hÃ¬nh áº£nh Ä‘á»™ sÃ¢u, sau Ä‘Ã³ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng cá»§a cáº£ hai hÃ¬nh áº£nh má»™t cÃ¡ch Ä‘á»™c láº­p. Cuá»‘i cÃ¹ng, cÃ¡c Ä‘áº·c trÆ°ng cá»§a RGB vÃ  Ä‘á»™ sÃ¢u Ä‘Æ°á»£c káº¿t há»£p Ä‘á»ƒ dá»± Ä‘oÃ¡n ID khuÃ´n máº·t.

6. LiÃªn quan Ä‘áº¿n áº£nh khÃ´ng gÃ¡n nhÃ£n => Bá» qua.

### 4.2.2. Embedding refinement by extra representations (tinh chá»‰nh embedding báº±ng cÃ¡c tham sá»‘ phá»¥ khÃ¡c)

2 giáº£i phÃ¡p [85] vÃ  [86] á»Ÿ trÃªn Ä‘á»u coi face embedding nhÆ° low-rank (tá»©c embedding cÃ³ Ã­t chiá»u). Há» chá»‰ thÃªm nhiá»…u vÃ o cÃ¡c image giÃºp cáº£i thiá»‡n kháº£ nÄƒng há»c cá»§a mÃ´ hÃ¬nh (giÃºp mÃ´ hÃ¬nh cÃ³ thá»ƒ bá»n vá»¯ng trÆ°á»›c nhá»¯ng tÃ¡c Ä‘á»™ng nhá» bÃªn ngoÃ i). Viá»‡c nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh hai pháº§n: tÃ¡i cáº¥u trÃºc cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t má»™t cÃ¡ch tuyáº¿n tÃ­nh tá»« má»™t tá»« Ä‘iá»ƒn (dictionary) vÃ  Ã¡p dá»¥ng cÃ¡c rÃ ng buá»™c thÆ°a (sparsity constraints).

1. **Neural Aggregation Network - NAN**
    **PhÆ°Æ¡ng phÃ¡p FR báº±ng video Ä‘iá»ƒn hÃ¬nh** báº±ng cÃ¡ch thao tÃºng cÃ¡c face embedding.

    Yang et al. Ä‘á» xuáº¥t ráº±ng, cÃ¡c hÃ¬nh áº£nh khuÃ´n máº·tcá»§a cÃ¹ng 1 IDs trong má»™t video vá»›i cÃ¹ng má»™t ID nÃªn Ä‘Æ°á»£c gá»™p láº¡i Ä‘á»ƒ xÃ¢y dá»±ng má»™t embedding máº¡nh máº½ hÆ¡n (robots embedding).

    ![](images/4.2.2.%20NAN.png)

2. **Dynamic Feature Matching**
    **PhÆ°Æ¡ng phÃ¡p nÃ y nháº±m giáº£i quyáº¿t váº¥n Ä‘á» nháº­n diá»‡n khuÃ´n máº·t khi chá»‰ cÃ³ má»™t pháº§n cá»§a khuÃ´n máº·t Ä‘Æ°á»£c hiá»ƒn thá»‹**, do che khuáº¥t hoáº·c do gÃ³c nhÃ¬n khÃ´ng thuáº­n lá»£i.

    Äáº§u tiÃªn, má»™t máº¡ng nÆ¡-ron tÃ­ch cháº­p Ä‘Æ°á»£c Ã¡p dá»¥ng Ä‘á»ƒ trÃ­ch xuáº¥t cÃ¡c Ä‘áº·c trÆ°ng tá»« hÃ¬nh áº£nh khuÃ´n máº·t probe (hÃ¬nh áº£nh cáº§n nháº­n diá»‡n) vÃ  hÃ¬nh áº£nh trong bá»™ sÆ°u táº­p (gallery) cÃ³ kÃ­ch thÆ°á»›c tÃ¹y Ã½. HÃ¬nh áº£nh probe Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  ğ‘ vÃ  hÃ¬nh áº£nh gallery Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  ğ‘”ğ‘ (vá»›i ğ‘ lÃ  nhÃ£n cá»§a hÃ¬nh áº£nh gallery).

    ThÃ´ng thÆ°á»ng, viá»‡c tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a ğ‘ vÃ  ğ‘”ğ‘ gáº·p khÃ³ khÄƒn do kÃ­ch thÆ°á»›c cáº£ 2 feature khÃ´ng nháº¥t quÃ¡n. Äá»ƒ kháº¯c phá»¥c Ä‘iá»u nÃ y, má»™t cá»­a sá»• trÆ°á»£t (sliding window) cÃ³ kÃ­ch thÆ°á»›c giá»‘ng nhÆ° ğ‘ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ phÃ¢n tÃ¡ch ğ‘”ğ‘â€‹ thÃ nh ğ‘˜ feature con: ğºğ‘=[ğ‘”ğ‘1,ğ‘”ğ‘2,â€¦,ğ‘”ğ‘ğ‘˜].

3. Pose Invariant Model (PIM)
    Má»™t phÆ°Æ¡ng phÃ¡p Ä‘Æ°á»£c Zhao et al. Ä‘á» xuáº¥t nháº±m cáº£i thiá»‡n viá»‡c nháº­n diá»‡n khuÃ´n máº·t (Face Recognition - FR) trong mÃ´i trÆ°á»ng tá»± nhiÃªn, khi khuÃ´n máº·t cÃ³ thá»ƒ bá»‹ nghiÃªng hoáº·c khÃ´ng Ä‘á»‘i diá»‡n trá»±c tiáº¿p vá»›i mÃ¡y áº£nh.

    PIM Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ lÃ m cho viá»‡c nháº­n diá»‡n khuÃ´n máº·t trá»Ÿ nÃªn báº¥t biáº¿n vá»›i gÃ³c nhÃ¬n (pose invariant), giÃºp nháº­n diá»‡n khuÃ´n máº·t hiá»‡u quáº£ ngay cáº£ khi gÃ³c nhÃ¬n cá»§a khuÃ´n máº·t khÃ´ng pháº£i lÃ  chÃ­nh diá»‡n.

    Máº¡ng con Face Frontalization Sub-Net (FFSN): Ä‘á»ƒ chuyá»ƒn Ä‘á»•i cÃ¡c hÃ¬nh áº£nh khuÃ´n máº·t nghiÃªng thÃ nh hÃ¬nh áº£nh khuÃ´n máº·t chÃ­nh diá»‡n.
        HÃ¬nh áº£nh khuÃ´n máº·t nghiÃªng ban Ä‘áº§u Ä‘Æ°á»£c Ä‘Æ°a vÃ o má»™t bá»™ phÃ¡t hiá»‡n cÃ¡c Ä‘iá»ƒm Ä‘áº·c trÆ°ng trÃªn khuÃ´n máº·t Ä‘á»ƒ xÃ¡c Ä‘á»‹nh cÃ¡c máº£ng Ä‘áº·c trÆ°ng (landmark patches).
        CÃ¡c máº£ng Ä‘áº·c trÆ°ng tá»« hÃ¬nh áº£nh khuÃ´n máº·t nghiÃªng nÃ y, gá»i chung lÃ  ğ¼ğ‘¡ğ‘Ÿâ€‹ , Ä‘Æ°á»£c Ä‘Æ°a vÃ o PIM.
        Sau Ä‘Ã³, PIM sá»­ dá»¥ng má»™t cáº¥u trÃºc mÃ£ hÃ³a-giáº£i mÃ£ (encoder-decoder) Ä‘Æ°á»£c kÃ½ hiá»‡u lÃ  ğº Ä‘á»ƒ táº¡o ra hÃ¬nh áº£nh khuÃ´n máº·t chÃ­nh diá»‡n tá»« ğ¼ğ‘¡ğ‘Ÿâ€‹, kÃ½ hiá»‡u lÃ  ğ¼â€²=ğº(ğ¼ğ‘¡ğ‘Ÿ).
        TÆ°Æ¡ng tá»± nhÆ° GAN, má»™t máº¡ng há»c phÃ¢n biá»‡t (discriminative learning sub-net) Ä‘Æ°á»£c káº¿t ná»‘i vá»›i FFSN nháº±m Ä‘áº£m báº£o ráº±ng hÃ¬nh áº£nh khuÃ´n máº·t chÃ­nh diá»‡n táº¡o ra, ğ¼â€², trÃ´ng giá»‘ng nhÆ° má»™t khuÃ´n máº·t thá»±c vÃ  mang thÃ´ng tin vá» danh tÃ­nh.

        Káº¿t quáº£: CÃ¡c Ä‘áº·c trÆ°ng tá»« khuÃ´n máº·t nghiÃªng vÃ  khuÃ´n máº·t chÃ­nh diá»‡n Ä‘Æ°á»£c táº¡o ra sáº½ Ä‘Æ°á»£c káº¿t há»£p Ä‘á»ƒ cÃ³ Ä‘Æ°á»£c má»™t biá»ƒu diá»…n khuÃ´n máº·t (face representation) tá»‘t hÆ¡n, giÃºp tÄƒng Ä‘á»™ chÃ­nh xÃ¡c khi nháº­n diá»‡n khuÃ´n máº·t.

### 4.2.3. Multi-task modeling with FR

BÃªn cáº¡nh Face Ids, ráº¥t nhiá»u cÃ¡c phÆ°Æ¡ng phÃ¡p chá»n Ä‘Æ°a thÃªm nhiá»u supervised information trong quÃ¡ trÃ¬nh train mÃ´ hÃ¬nh FR.

1. **Peng et al. [103]** giá»›i thiá»‡u 1 phÆ°Æ¡ng phÃ¡p há»c biá»ƒu diá»…n cÃ¡c feature sao cho nÃ³ khÃ´ng phá»¥ thuá»™c vÃ o tÆ° tháº¿ cá»§a khuÃ´n máº·t **pose-invariant**.
    Äáº§u tiÃªn 3D facial model sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ táº¡o ra áº£nh cÃ³ gÃ³c nhÃ¬n má»›i so vá»›i gÃ³c nhÃ¬n gáº§n hÆ°á»›ng chÃ­nh diá»‡n ban Ä‘áº§u.

    NgoÃ i ID cá»§a áº£nh, cÃ²n cÃ³ cÃ¡c thÃ´ng tin khÃ¡c nhÆ° face pose (tÆ° tháº¿ khuÃ´n máº·t), landmark (Ä‘áº·c Ä‘iá»ƒm khuÃ´n máº·t nhÆ° tai mÅ©i há»ng, ...) cÅ©ng Ä‘Æ°á»£c Ä‘Æ°a vÃ o giÃ¡m sÃ¡t trong quÃ¡ trÃ¬nh train. GiÃºp mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c Ä‘áº·c trÆ°ng phong phÃº hÆ¡n báº±ng cÃ¡ch há»c Ä‘á»“ng thá»i cáº£ Ä‘áº·c trÆ°ng nháº­n dáº¡ng vÃ  Ä‘áº·c trÆ°ng khÃ´ng liÃªn quan Ä‘áº¿n nháº­n dáº¡ng vá»›i bá»™ trÃ­ch xuáº¥t Ä‘áº·c trÆ°ng (extractor Î¸áµ£).
        ei: ID labels
        ep: pose labels
        ei: landmark labels
    
    Rich embedding (1 thuáº­t ngá»¯ Ä‘á»ƒ chá»‰ ráº±ng embedding chá»©a phong phÃº cÃ¡c thÃ´ng tin) trong quÃ¡ trÃ¬nh train sáº½ Ä‘Æ°á»£c chia lÃ m cÃ¡c feature vá» identity, pose vÃ  landmark feature. VÃ  Ä‘á»ƒ train ra cÃ¡c feature nÃ y, ta sáº½ sá»­ dá»¥ng cÃ¡c loss khÃ¡c nhau. 
        softmax cho viá»‡c Æ°á»›c tÃ¬nh ID
        L2 regression Ä‘Æ°á»£c sá»­ dá»¥ng cho pose vÃ  landmark preidict.
    
    Cuá»‘i cÃ¹ng 1 cáº·p áº£nh: áº£nh cÃ³ tÆ° tháº¿ gáº§n chÃ­nh diá»‡n x1 vÃ  1 áº£nh cÃ³ tÆ° tháº¿ khÃ´ng pháº£i chÃ­nh diá»‡n x2 Ä‘Æ°á»£c Ä‘Æ°a vÃ o máº¡ng recognition Î¸r Ä‘á»ƒ trÃ­ch xuáº¥t embedding er1 vÃ  er2.

    Há» sá»­ dá»¥ng ká»¹ thuáº­t disentangling (tÃ¡ch biá»‡t) dá»±a trÃªn reconstruction Ä‘á»ƒ  cháº¯t lá»c nhá»¯ng feature liÃªn quan Ä‘áº¿n identity feature khá»i nhá»¯ng feature khÃ´ng liÃªn quan Ä‘áº¿n identity.
    
    Káº¿t qá»§a sau cÃ¹ng ta thu Ä‘Æ°á»£c 1 embedding Ä‘Æ°á»£c biá»ƒu diá»…n khÃ´ng phá»¥ thuá»™c vÃ o pose cá»§a khuÃ´n máº·t, giÃºp cáº£i thiá»‡n kháº£ nÄƒng nháº­n dáº¡ng trong cÃ¡c tÃ­nh huá»‘ng máº·t cÃ³ pose khÃ¡c nhau.

    ![](images/4.2.3.%20chá»‹u.png)

2. **Wang et al. [104]** giáº£i quyáº¿t váº¥n Ä‘á» há»‡ thá»‘ng nháº­n diá»‡n khÃ´ng phá»¥ thuá»™c vÃ o tuá»•i báº±ng cÃ¡ch thÃªm 1 task predict age (dá»¯ liá»‡u dÃ¹ng Ä‘á»ƒ train lÃ  áº£nh cá»§a ngÆ°á»i Ä‘Ã³ táº¡i 1 Ä‘á»™ tuá»•i khÃ´ng Ä‘á»•i trong khi cÃ¡c Ä‘áº·c trÆ°ng cá»§a khuÃ´n máº·t cÃ³ thá»ƒ thay Ä‘á»•i theo thá»i gian). 
    NhÃ³m tÃ¡c giáº£ Ä‘á» xuáº¥t Orthogonal Embedding CNNs (OE-CNNs) nháº±m há»c cÃ¡c Ä‘áº·c trÆ°ng khuÃ´n máº·t khÃ´ng phá»¥ thuá»™c vÃ o tuá»•i tÃ¡c.
    Äáº§u tiÃªn há» train 1 mÃ´ hÃ¬nh face feature extractor Ä‘á»ƒ thu Ä‘Æ°á»£c feature xi cá»§a sample i. Sau Ä‘Ã³ xi sáº½ Ä‘Æ°á»£c tÃ¡ch lÃ m 2 thÃ nh pháº§n:
        ThÃ nh pháº§n liÃªn quan Ä‘áº¿n danh tÃ­nh xid: ÄÆ°á»£c tá»‘i Æ°u bá»Ÿi SphereFace[47]
        ThÃ nh pháº§n liÃªn quan Ä‘áº¿n Ä‘á»™ tuá»•i xage: ÄÆ°á»£c tá»‘i Æ°u bá»i hÃ m loss sau.
            ![](images/4.2.3.%20Loss%20age%20OE%20CNN.png)

            Trong Ä‘Ã³ ||xi||2 lÃ  Ä‘á»™ dÃ i cá»§a embedding xi, zi lÃ  label vá» tuá»•i. M lÃ  batch size.

3. **Liu et al. [105]** há»£p nháº¥t 3D face reconstruction (tÃ¡i táº¡o khuÃ´n máº·t 3D) vÃ  recognition (nháº­n dáº¡ng khuÃ´n máº·t).
    Sá»­ dá»¥ng dá»¯ liá»‡u point cloud.
        ![](images/4.2.3.%20point%20cloud.png)
    NhÃ³m tÃ¡c giáº£ cho ráº±ng 3D shape cÃ³ thá»ƒ Ä‘Æ°á»£c chia thÃ nh cÃ¡c thÃ nh pháº§n liÃªn quan Ä‘áº¿n identity hoáº·c khÃ´ng liÃªn quan Ä‘áº¿n identity.
        ![](images/4.2.3.%20cÃ¡c%20thÃ nh%20pháº§n%20cá»§a%20máº·t%203d.png)
    Sau Ä‘Ã³ 1 encoder Ä‘Æ°á»£c built Ä‘á»ƒ extract face feature tá»« 1 áº£nh 2D. Feature nÃ y Ä‘Æ°á»£c chia thÃ nh 2 thÃ nh pháº§n cid (cÃ¡c feature liÃªn quan Ä‘áº¿n Ä‘á»‹nh danh, phá»¥c vá»¥ cho viá»‡c nhÃ¢n dáº¡ng) vÃ  cres (cÃ¡c feature mÃ´ táº£ hÃ¬nh dáº¡ng khuÃ´n máº·t, khÃ´ng liÃªn quan Ä‘áº¿n danh tÃ­nh):
        ![](images/4.2.3.%20Liu%20encoder.png)
    CÃ¡c hÃ m loss LC, LR Ä‘Æ°á»£c thiáº¿t káº¿ cho viá»‡c predict identity vÃ  recontruction 3D shape.
        LC lÃ  softmax Ä‘á»ƒ tá»‘i Æ°u cid
    Tá»•ng quan kiáº¿n trÃºc máº¡ng multi task 
        ![](images/4.2.3.%20TÃ¡i%20táº¡o%203D.png)

4. **Wang et al. [106]** Ä‘á» xuáº¥t máº¡ng FM2u-Net Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ táº¡o ra khuÃ´n máº·t cÃ³ trang Ä‘iá»ƒm, nháº±m há»— trá»£ cÃ¡c tÃ¡c vá»¥ xÃ¡c thá»±c khuÃ´n máº·t khÃ´ng phá»¥ thuá»™c vÃ o trang Ä‘iá»ƒm.
    Kiáº¿n trÃºc FM2u-Net
        **FM-Net (Face Morphological Network): DÃ¹ng Ä‘á»ƒ táº¡o ra khuÃ´n máº·t cÃ³ trang Ä‘iá»ƒm.**
            Cycle consistent loss: ÄÆ°á»£c sá»­ dá»¥ng Ä‘á»ƒ hÆ°á»›ng dáº«n quÃ¡ trÃ¬nh huáº¥n luyá»‡n táº¡o áº£nh trang Ä‘iá»ƒm giá»‘ng tháº­t.
            Dá»¯ liá»‡u huáº¥n luyá»‡n: Do thiáº¿u cÃ¡c cáº·p dá»¯ liá»‡u cÃ³ vÃ  khÃ´ng cÃ³ trang Ä‘iá»ƒm, FM-Net sá»­ dá»¥ng áº£nh gá»‘c vÃ  cÃ¡c miáº¿ng vÃ¡ khuÃ´n máº·t (facial patches) lÃ m thÃ´ng tin giÃ¡m sÃ¡t.
            Háº¡n cháº¿ thay Ä‘á»•i danh tÃ­nh:
                ThÃªm softmax loss Ä‘á»ƒ dá»± Ä‘oÃ¡n ID.
                ThÃªm ID-preserving loss Ä‘á»ƒ Ä‘áº£m báº£o khuÃ´n máº·t táº¡o ra váº«n giá»¯ Ä‘Æ°á»£c danh tÃ­nh ban Ä‘áº§u.
        **AttM-Net (Attention Makeup Network): DÃ¹ng Ä‘á»ƒ há»c Ä‘áº·c trÆ°ng khuÃ´n máº·t khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi trang Ä‘iá»ƒm.**
            gá»“m:
                Má»™t nhÃ¡nh toÃ n cá»¥c (global branch).
                Ba nhÃ¡nh cá»¥c bá»™ (local branches): Táº­p trung vÃ o hai máº¯t vÃ  miá»‡ng.
            Má»¥c Ä‘Ã­ch: Há»c Ä‘áº·c trÆ°ng tá»•ng quÃ¡t vÃ  chi tiáº¿t cá»§a cÃ¡c pháº§n khuÃ´n máº·t, tá»« Ä‘Ã³ táº¡o ra Ä‘áº·c trÆ°ng khÃ´ng bá»‹ áº£nh hÆ°á»Ÿng bá»Ÿi trang Ä‘iá»ƒm.
            
            Káº¿t há»£p Ä‘áº·c trÆ°ng (Feature Fusion): Káº¿t há»£p Ä‘áº·c trÆ°ng cá»§a cÃ¡c pháº§n khÃ¡c nhau láº¡i thÃ nh má»™t Ä‘áº·c trÆ°ng tá»•ng há»£p fcls
            
            HÃ m loss:â€‹               
                ![](images/4.2.3.%20attm%20loss.png)

5. **Gong et al. [108]** Ä‘á» xuáº¥t 1 máº¡ng Ä‘á»‘i nghá»‹ch khá»­ thiÃªn vá»‹ (DebFace) Ä‘á»ƒ cÃ¹ng há»c FR vÃ  cÃ¡c thuá»™c tÃ­nh nhÃ¢n kháº©u há»c nhÆ° giá»›i tÃ­nh, tuá»•i vÃ  chá»§ng tá»™c.
    DepFace network gá»“m 4 thÃ nh pháº§n.
        Encoder Ä‘á»ƒ trÃ­ch xuáº¥t feature tá»« image:
        CÃ¡c bá»™ classifer bao gá»“m: CG Ä‘á»ƒ phÃ¢n loáº¡i giá»›i tÃ­nh, CA Ä‘á»ƒ phÃ¢n loáº¡i tuá»•i, CR Ä‘á»ƒ phÃ¢n loáº¡i chá»§ng tá»™c vÃ  CID Ä‘á»ƒ phÃ¢n loáº¡i danh tÃ­nh
        Bá»™ phÃ¢n loáº¡i phÃ¢n phá»‘i CDistr
        Máº¡ng tá»•ng há»£p cÃ¡c feature sau cÃ¹ng EF eat
    
## 4.3. FR with massive IDs
CÃ ng nhiá»u dataset sáº½ giÃºp há»‡ thá»‘ng FR tá»‘t hÆ¡n, tuy nhiÃªn Ä‘i kÃ¨m vá»›i nÃ³ lÃ  nhá»¯ng váº¥n Ä‘á», thÃ¡ch thá»©c má»›i. Chi phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»›: Sá»‘ lÆ°á»£ng ID lá»›n trong táº­p huáº¥n luyá»‡n cÃ³ thá»ƒ cáº£i thiá»‡n káº¿t quáº£ FR, nhÆ°ng Ä‘á»“ng thá»i cÅ©ng lÃ m tÄƒng chi phÃ­ tÃ­nh toÃ¡n vÃ  bá»™ nhá»›. Viá»‡c má»Ÿ rá»™ng sá»‘ lÆ°á»£ng lá»›p phÃ¢n loáº¡i cÃ³ thá»ƒ vÆ°á»£t quÃ¡ kháº£ nÄƒng cá»§a GPU, dáº«n Ä‘áº¿n viá»‡c cáº§n cÃ¡c giáº£i phÃ¡p Ä‘á»ƒ tá»‘i Æ°u hÃ³a.

1.  Partial FC [112][113]
    Giáº£i phÃ¡p Ä‘áº§u tiÃªn Ä‘Æ°á»£c Ä‘á» xuáº¥t lÃ  phÃ¢n tÃ¡ch lá»›p phÃ¢n loáº¡i theo chiá»u lá»›p vÃ  phÃ¢n phá»‘i Ä‘á»u trÃªn cÃ¡c GPU. Partial FC Ä‘á» xuáº¥t ráº±ng khÃ´ng cáº§n sá»­ dá»¥ng táº¥t cáº£ cÃ¡c trung tÃ¢m lá»›p Ã¢m khi tÃ­nh toÃ¡n logits; chá»‰ cáº§n láº¥y máº«u má»™t pháº§n trung tÃ¢m lá»›p Ã¢m cÅ©ng Ä‘á»§ Ä‘á»ƒ Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ chÃ­nh xÃ¡c tÆ°Æ¡ng Ä‘á»‘i cao.

2. BroadFace: Äá» xuáº¥t cá»§a BroadFace lÃ  giá»¯ láº¡i cÃ¡c Ä‘áº·c trÆ°ng cá»§a cÃ¡c vÃ²ng láº·p trÆ°á»›c Ä‘Ã³ trong má»™t hÃ ng Ä‘á»£i vÃ  tá»‘i Æ°u hÃ³a cÃ¡c tham sá»‘ cá»§a lá»›p phÃ¢n loáº¡i cÃ¹ng vá»›i cÃ¡c Ä‘áº·c trÆ°ng hiá»‡n táº¡i, nháº±m nÃ¢ng cao sá»‘ lÆ°á»£ng máº«u tham gia vÃ o má»—i láº§n tá»‘i Æ°u hÃ³a.

3. Váº¥n Ä‘á» vá» khÃ´ng gian Ä‘áº·c trÆ°ng: Khi tiáº¿n trÃ¬nh huáº¥n luyá»‡n diá»…n ra, khÃ´ng gian Ä‘áº·c trÆ°ng cá»§a mÃ´ hÃ¬nh cÃ³ thá»ƒ trÃ´i dáº¡t, táº¡o ra khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘áº·c trÆ°ng trong hÃ ng Ä‘á»£i vÃ  khÃ´ng gian Ä‘áº·c trÆ°ng hiá»‡n táº¡i. Äá»ƒ kháº¯c phá»¥c, cÃ¡c tham sá»‘ cá»§a lá»›p phÃ¢n loáº¡i á»Ÿ vÃ²ng láº·p trÆ°á»›c Ä‘Ã³ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u chá»‰nh cÃ¡c Ä‘áº·c trÆ°ng trong hÃ ng Ä‘á»£i.

4. Äá» xuáº¥t má»›i (Virtual FC layer): Má»™t lá»›p má»›i gá»i lÃ  Virtual FC layer Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£m thiá»ƒu má»©c tiÃªu thá»¥ tÃ­nh toÃ¡n. NÃ³ chia sá»‘ lÆ°á»£ng ID trong táº­p huáº¥n luyá»‡n thÃ nh cÃ¡c nhÃ³m vÃ  sá»­ dá»¥ng má»™t ma tráº­n chiáº¿u Ä‘á»ƒ chia sáº» cÃ¡c cá»™t giá»¯a cÃ¡c nhÃ³m.

5. Faster Face Classification (F2C): Má»™t phÆ°Æ¡ng phÃ¡p má»›i Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ lÆ°u trá»¯ vÃ  cáº­p nháº­t cÃ¡c Ä‘áº·c trÆ°ng cá»§a danh tÃ­nh má»™t cÃ¡ch Ä‘á»™ng, cÃ³ thá»ƒ Ä‘Æ°á»£c xem lÃ  má»™t sá»± thay tháº¿ cho lá»›p phÃ¢n loáº¡i thÃ´ng thÆ°á»ng

## 4.4. Cross domain in FR
NhÃ¬n chung khi sá»­ dá»¥ng cÃ¡c thuáº­t toÃ¡n FR, training set vÃ  testing set thÆ°á»ng trong 1 bá»‘i cáº£nh giá»‘ng nhau (similar distribution). Tuy nhiÃªn face images tá»« cÃ¡c chá»§ng tá»‘c khÃ¡c nhau, trong cÃ¡c bá»‘i cáº£nh khÃ¡c nhau (mobile photo albums, online video, CÄƒn cÆ°á»›c cÃ´ng dÃ¢n, ...) Ä‘á»u cÃ³ Ä‘á»™ lá»‡ch miá»n rÃµ rÃ ng (domain bias). Hiá»‡u suáº¥t model sáº½ giáº£m máº¡nh khi training set vÃ  testing set cÃ³ khoáº£ng cÃ¡ch miá»n. Äiá»u nÃ y dáº«n Ä‘áº¿n kháº£ nÄƒng tá»•ng quÃ¡t hÃ³a kÃ©m cá»§a máº¡ng neutron trong viá»‡c xá»­ lÃ½ dá»¯ liá»‡u chÆ°a tá»«ng tháº¥y trong thá»±c táº¿. Do Ä‘Ã³ cÃ¡c giáº£i phÃ¡p thÃ­ch á»©ng miá»n Ä‘Ã£ Ä‘Æ°á»£c Ä‘á» xuáº¥t Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» nÃ y.

Pháº§n nÃ y nÃ³i vá» cÃ¡c giáº£i phÃ¡p thÃ­ch á»©ng miá»n chung (general domain adaptation), sau Ä‘Ã³ liá»‡t kÃª 1 sá»‘ phÆ°Æ¡ng phÃ¡p Ã¡p dá»¥ng cho cÃ¡c táº­p training set Ä‘áº·c biá»‡t

1. MAML (Model-Agnostic Meta-Learning): Ä‘Æ°á»£c ráº¥t nhiá»u researcher sá»­ dá»¥ng Ä‘á»ƒ giáº£i quyáº¿t cross domain.
    Guo et al. [119] Ä‘Ã£ Ä‘á» xuáº¥t 1 phÆ°Æ¡ng phÃ¡p nháº­n diá»‡n khuÃ´n máº·t meta (MFR - Meta Face Recognition) Ä‘á»ƒ xá»­ lÃ½ váº¥n Ä‘á» nÃ y.

    Trong má»—i vÃ²ng láº·p huáº¥n luyá»‡n, chá»‰ má»™t trong sá»‘ N miá»n trong táº­p huáº¥n luyá»‡n Ä‘Æ°á»£c chá»n lÃ m dá»¯ liá»‡u thá»­ nghiá»‡m meta (meta-test data), trong khi N-1 miá»n cÃ²n láº¡i Ä‘Æ°á»£c sá»­ dá»¥ng lÃ m dá»¯ liá»‡u huáº¥n luyá»‡n meta (meta-train data). Táº¥t cáº£ cÃ¡c dá»¯ liá»‡u nÃ y táº¡o thÃ nh má»™t meta-batch. Dá»¯ liá»‡u thá»­ nghiá»‡m meta Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ mÃ´ phá»ng hiá»‡n tÆ°á»£ng chuyá»ƒn miá»n trong cÃ¡c ká»‹ch báº£n á»©ng dá»¥ng.

    Ba loáº¡i hÃ m máº¥t mÃ¡t Ä‘Æ°á»£c Ä‘á» xuáº¥t bao gá»“m:

        HÃ m máº¥t mÃ¡t attention hard-pair (Lhp): Tá»‘i Æ°u hÃ³a cÃ¡c cáº·p tÃ­ch cá»±c vÃ  tiÃªu cá»±c báº±ng cÃ¡ch giáº£m khoáº£ng cÃ¡ch Euclidean giá»¯a cÃ¡c cáº·p tÃ­ch cá»±c khÃ³ vÃ  Ä‘áº©y cÃ¡c cáº·p tiÃªu cá»±c khÃ³ ra xa.
        HÃ m máº¥t mÃ¡t phÃ¢n loáº¡i má»m (Lcls): DÃ¹ng cho phÃ¢n loáº¡i ID khuÃ´n máº·t, Ä‘Æ°á»£c Ä‘iá»u chá»‰nh tá»« hÃ m máº¥t mÃ¡t cross-entropy.
        HÃ m máº¥t mÃ¡t Ä‘á»‹nh hÆ°á»›ng miá»n (Lda): Thiáº¿t káº¿ Ä‘á»ƒ lÃ m cho cÃ¡c vector nhÃºng trung bÃ¬nh cá»§a nhiá»u miá»n huáº¥n luyá»‡n meta gáº§n nhau hÆ¡n.

2. Faraki vÃ  cÃ¡c cá»™ng sá»± nháº­n tháº¥y váº¥n Ä‘á» trong sá»­ dá»¥ng hÃ m máº¥t mÃ¡t Ä‘á»‹nh hÆ°á»›ng miá»n (domain aligment loss, Lda) trong phÆ°Æ¡ng phÃ¡p MFR.
    Há» nháº­n tháº¥y ráº±ng viá»‡c kÃ©o gáº§n giÃ¡ trá»‹ trung bÃ¬nh cá»§a cÃ¡c miá»n cÃ³ thá»ƒ dáº«n Ä‘áº¿n giáº£m hiá»‡u suáº¥t cá»§a mÃ´ hÃ¬nh, vÃ¬ khi Ä‘Ã³ cÃ¡c máº«u thuá»™c cÃ¡c ID khÃ¡c nhau cÃ³ thá»ƒ bá»‹ kÃ©o gáº§n láº¡i vá»›i nhau, lÃ m giáº£m Ä‘á»™ chÃ­nh xÃ¡c.

    Äá»ƒ kháº¯c phá»¥c váº¥n Ä‘á» nÃ y, Faraki et al. Ä‘Ã£ Ä‘á» xuáº¥t má»™t hÃ m máº¥t mÃ¡t má»›i gá»i lÃ  cross domain triplet loss (máº¥t mÃ¡t ba pháº§n trong cÃ¡c miá»n khÃ¡c nhau), dá»±a trÃªn hÃ m máº¥t mÃ¡t triplet. CÃ´ng thá»©c cho hÃ m máº¥t mÃ¡t nÃ y Ä‘Æ°á»£c mÃ´ táº£ trong Ä‘oáº¡n vÄƒn vÃ  bao gá»“m cÃ¡c thÃ nh pháº§n nhÆ° sau:
        CÃ¡c triplet: Gá»“m ba pháº§n: anchor (máº«u gá»‘c), positive (máº«u tÃ­ch cá»±c) vÃ  negative (máº«u tiÃªu cá»±c) trong khÃ´ng gian Ä‘áº·c trÆ°ng.
        Khoáº£ng cÃ¡ch Mahalanobis: ÄÆ°á»£c tÃ­nh toÃ¡n dá»±a trÃªn cÃ¡c cáº·p tÃ­ch cá»±c trong miá»n, nháº±m Ä‘iá»u chá»‰nh khoáº£ng cÃ¡ch giá»¯a cÃ¡c máº«u trong cÃ¡c miá»n khÃ¡c nhau.   
    
    HÃ m máº¥t mÃ¡t ba pháº§n trong cÃ¡c miá»n khÃ¡c nhau giÃºp cÄƒn chá»‰nh phÃ¢n phá»‘i giá»¯a cÃ¡c miá»n khÃ¡c nhau, nháº±m cáº£i thiá»‡n Ä‘á»™ chÃ­nh xÃ¡c cá»§a mÃ´ hÃ¬nh.

... tá»± Ä‘á»c chá»‹u cháº¯c khÃ´ng dÃ¹ng ...

## 4.5. FR pipeline acceleration (tÄƒng tá»‘c Ä‘á»™ xá»­ lÃ½ pipeline cá»§a há»‡ thá»‘ng FR)

1. Network Slimming:
    ÄÆ°á»£c giá»›i thiá»‡u bá»Ÿi Liu et al. Ã tÆ°á»Ÿng chÃ­nh cá»§a phÆ°Æ¡ng phÃ¡p xoay quay scaling factor trong má»‘i bÆ°á»›c batch normalization (BN) cÃ³ thá»ƒ chá»‰ ra má»©c Ä‘á»™ quan trá»ng cá»§a má»—i channel. TÃ¡c giáº£ sá»­ dá»¥ng L1 regularization dá»±a trÃªn scaling factor trong BN layer trong quÃ¡ trÃ¬nh train Ä‘á»ƒ lÃ m nÃ³ thÆ°a thá»›t, sau Ä‘Ã³ cáº¯t bá» cÃ¡c kÃªnh khÃ´ng quan trá»ng theo size cá»§a scaling factor.
        scaling factor Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u chá»‰nh Ä‘á»™ lá»›n Ä‘áº§u ra cá»§a má»—i kÃªnh (channel) trong máº¡ng neutron. Cá»¥ thá»ƒ, sau khi Ä‘áº§u vÃ o cá»§a má»™t kÃªnh Ä‘Æ°á»£c chuáº©n hÃ³a trong lá»›p BN, nÃ³ sáº½ Ä‘Æ°á»£c nhÃ¢n vá»›i há»‡ sá»‘ tá»‰ lá»‡ nÃ y Ä‘á»ƒ Ä‘iá»u chá»‰nh giÃ¡ trá»‹ Ä‘áº§u ra.
        VÃ­ dá»¥ 1 máº¡ng CNN tiáº¿p ná»‘i bá»Ÿi 1 lá»›p BN.
            Convolutional Layer: 10 kÃªnh (filters), kÃ­ch thÆ°á»›c kernel lÃ  3Ã—3.
            Batch Normalization Layer: Ä‘i sau lá»›p convolution Ä‘á»ƒ chuáº©n hÃ³a Ä‘áº§u ra cá»§a cÃ¡c kÃªnh.
            Fully Connected Layer (hoáº·c cÃ³ thá»ƒ nhiá»u lá»›p convolution khÃ¡c).
    Káº¿t quáº£ thá»±c nghiá»‡m cho tháº¥y, structure cá»§a máº¡ng quan trá»ng hÆ¡n weight vÃ  viá»‡c cáº¯t tá»‰a Ä‘Æ°á»£c coi lÃ  quÃ¡ trÃ¬nh tÃ¬m kiáº¿m structure máº¡ng thÃ­ch há»£p. Sau khi cáº¯t tá»‰a, viá»‡c train máº¡ng vá»›i 1 chuá»—i cÃ¡c tham sá»‘ khá»Ÿi táº¡o báº¥t ká»³ sáº½ cho máº¡ng Ä‘áº¡t káº¿t quáº£ tá»‘t hÆ¡n.

2. K-D tree
    Trong váº¥n Ä‘á» face identication, chÃºng ta cáº§n khá»›p Ä‘áº·c Ä‘iá»ƒm face feature cá»§a input vá»›i 1 face feature trong gallery. ÄÃ¢y lÃ  váº¥n Ä‘á» 1:n vÃ  Ä‘á» Ä‘áº©y nhanh quÃ¡ trÃ¬nh nÃ y K-D tree lÃ  1 thuáº­t toÃ¡n phá»• biáº¿n Ä‘Æ°á»£c sá»­ dá»¥ng.

    K-D tree liÃªn tá»¥c chia feature space thÃ nh 2 pháº§n Ä‘á»ƒ táº¡o thÃ nh cáº¥u trÃºc cÃ¢y nhá»‹ phÃ¢n. Má»—i point trÃªn feature space tÆ°Æ¡ng á»©ng vá»›i 1 node trong tree. Sau Ä‘Ã³ chá»‰ cáº§n cháº¡y thuáº­t toÃ¡n tÃ¬m kiáº¿m gáº§n nháº¥t

    Háº¡n cháº¿ cá»§a phÆ°Æ¡ng phÃ¡p: khi feature space cÃ³ nhiá»u chiá»u, dá»¯ liá»‡u thÆ°á»ng phÃ¢n bá»• ráº¥t thÆ°a thá»›t khiáº¿n cho K-D suy giáº£m hiá»‡u quáº£ (gáº§n nhÆ° trá»Ÿ thÃ nh tÃ¬m kiáº¿m tuyáº¿n tÃ­nh). Viá»‡c phÃ¢n Ä‘oáº¡n khÃ´ng gian Ä‘áº·c trÆ°ng kÃ©m hiá»‡u quáº£, dáº«n Ä‘áº¿n Ä‘á»™ chÃ­nh xÃ¡c cá»§a tÃ¬m kiáº¿m cÅ©ng giáº£m xuá»‘ng.

3. Vector Quantization vÃ  Product Quantization
    CÃ¡c thuáº­t toÃ¡n nÃ y mÃ£ hÃ³a khÃ´ng gian Ä‘áº·c trÆ°ng thÃ nh má»™t sá»‘ lÆ°á»£ng Ä‘iá»ƒm giá»›i háº¡n (codebook), giÃºp giáº£m sá»‘ láº§n tÃ­nh toÃ¡n trong viá»‡c so khá»›p Ä‘áº·c trÆ°ng. Product quantization tiáº¿p tá»¥c chia khÃ´ng gian Ä‘áº·c trÆ°ng thÃ nh cÃ¡c sub-vectors vÃ  Ã¡p dá»¥ng codebooks cho tá»«ng nhÃ³m sub-vectors.

4. Thuáº­t toÃ¡n tÃ¬m kiáº¿m dá»±a trÃªn Ä‘á»“ thá»‹
    Trong Ä‘Ã³, cÃ¡c Ä‘iá»ƒm trong khÃ´ng gian Ä‘áº·c trÆ°ng Ä‘Æ°á»£c káº¿t ná»‘i thÃ nh Ä‘á»“ thá»‹. NSW (Navigable Small World) lÃ  má»™t thuáº­t toÃ¡n tiÃªu biá»ƒu trong nhÃ³m nÃ y, giÃºp cáº£i thiá»‡n viá»‡c tÃ¬m kiáº¿m nhanh chÃ³ng nhá» vÃ o liÃªn káº¿t ngáº¯n vÃ  dÃ i, táº¡o ra cÃ¡c "Ä‘Æ°á»ng táº¯t" trong Ä‘á»“ thá»‹.

5. Locality Sensitive Hashing (LSH): 
    Thuáº­t toÃ¡n nÃ y sá»­ dá»¥ng hÃ m bÄƒm Ä‘á»ƒ phÃ¢n loáº¡i cÃ¡c vector gáº§n nhau thÃ nh cÃ¹ng má»™t giÃ¡ trá»‹ bÄƒm, giÃºp giáº£m sá»‘ lÆ°á»£ng vector cáº§n so sÃ¡nh trá»±c tiáº¿p, tá»« Ä‘Ã³ tÄƒng hiá»‡u quáº£ tÃ¬m kiáº¿m.

6. Knowledge Distillation
    ÄÃ¢y lÃ  phÆ°Æ¡ng phÃ¡p nÃ©n mÃ´ hÃ¬nh lá»›n (CNN) thÃ nh cÃ¡c mÃ´ hÃ¬nh nháº¹ hÆ¡n Ä‘á»ƒ phÃ¹ há»£p vá»›i thiáº¿t bá»‹ di Ä‘á»™ng. PhÆ°Æ¡ng phÃ¡p nÃ y bao gá»“m viá»‡c tá»‘i Æ°u hÃ³a vÃ  giáº£m thiá»ƒu Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¡c Ä‘áº·c trÆ°ng, Ä‘áº£m báº£o mÃ´ hÃ¬nh nhá» váº«n duy trÃ¬ Ä‘Æ°á»£c hiá»‡u nÄƒng tá»« mÃ´ hÃ¬nh gá»‘c.

## 4.6. Closed-set Training

FR thÆ°á»ng lÃ  váº¥n Ä‘á» clossed-set training. Closed-set training nghÄ©a lÃ  cÃ¡c áº£nh Ä‘Æ°á»£c sá»­ dá»¥ng test vá»›i modle sau khi train sáº½ cÃ³ IDs náº±m trong training sets. TÃ³m láº¡i nhá» vÃ o viá»‡c cÃ¡c IDs trong táº­p testset cÃ³ náº±m trong training set hay khÃ´ng ta cÃ³ thá»ƒ phÃ¢n loáº¡i há»‡ thá»‘ng FR thÃ nh closed-set systems vÃ  open-set systems.

Tong et al. [142] Ä‘Ã£ Ä‘á» xuáº¥t 1 framework Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ Ä‘á»™ há»‡ thá»‘ng FR vá»›i clossed-set vÃ  open-set. Káº¿t quáº£ thá»­ nghiá»‡m cho tháº¥y ráº±ng, há»‡ thá»‘ng FR táº­p má»Ÿ dá»… bá»‹ tá»•n thÆ°Æ¡ng hÆ¡n há»‡ thá»‘ng táº­p Ä‘Ã³ng dÆ°á»›i cÃ¡c loáº¡i táº¥n cÃ´ng khÃ¡c nhau (táº¥n cÃ´ng ká»¹ thuáº­t sá»‘, táº¥n cÃ´ng váº­t lÃ½ thá»±c hiá»‡n á»Ÿ cáº¥p Ä‘á»™ pixel vÃ  táº¥n cÃ´ng thá»¥c hiá»‡n á»Ÿ cáº¥p Ä‘á»™ grid). TÃ³m láº¡i closs-set training dá»… hÆ¡n vá»›i open-set training.

## 4.7. Mask face recognition
FR Ä‘Ã£ Ä‘áº¡t Ä‘Æ°á»£c nhá»¯ng tiáº¿n bá»™ Ä‘Ã¡ng ká»ƒ trong vÃ i nÄƒm gáº§n Ä‘Ã¢y. Tuy nhiÃªn, khi Ã¡p dá»¥ng mÃ´ hÃ¬nh FR Ä‘Ã³ vÃ o cÃ¡c tÃ¬nh huá»‘ng khÃ´ng bá»‹ háº¡n cháº¿, hiá»‡u suáº¥t nháº­n dáº¡ng khuÃ´n máº·t giáº£m máº¡nh, Ä‘áº·c biá»‡t lÃ  khi khuÃ´n máº·t bá»‹ che khuáº¥t bá»Ÿi kháº©u trang. CÃ¡c phÆ°Æ¡ng phÃ¡p FR hiá»‡n Ä‘áº¡i giáº£i quyáº¿t bÃ i toÃ¡n nÃ y thÆ°á»ng lÃ  biáº¿n thá»ƒ cá»§a 2 cÃ¡ch tiáº¿p cáº­n.
- KhÃ´i phá»¥c cÃ¡c pháº§n khuÃ´n máº·t bá»‹ che khuáº¥t - recovering occluÄ‘e facial parts [143, 144, 145, 146]
- Loáº¡i bá» cÃ¡c Ä‘áº·c Ä‘iá»ƒm bá»‹ há»ng do che khuáº¥t [147, 148, 149]: cÃ¡ch nÃ y thÆ°á»ng Ä‘Æ°á»£c Æ°u tiÃªn hÆ¡n vÃ¬ khÃ´ng cÃ³ gÃ¬ Ä‘áº£m báº£o viá»‡c khÃ´i phá»¥c diá»…n ra thuáº­n lá»£i.

1. Mask Decoder vÃ  Occlusion Pattern: Ä‘Æ°á»£c FROM[150] Ä‘á» xuáº¥t Ä‘á»ƒ dá»± Ä‘oÃ¡n pháº§n sample bá»‹ che khuáº¥t.

    ![](images/4.7.%20FROM.png)

    Kiáº¿n trÃºc nhÆ° trÃªn hÃ¬nh

    Äáº§u tiÃªn nÃ³ sáº½ láº¥y mini-batch images lÃ m Ä‘áº§u vÃ o. ThÃ´ng qua Feature Pyramid Extractor sáº½ thu Ä‘Æ°á»£c 3 scale feature maps nhÆ° X1, X2, X3. Trong Ä‘Ã³ X3 Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ giáº£i mÃ£ (decode) the mask - cÃ¡i mÃ  chá»©a thÃ´ng tin vá»‹ trÃ­ bá»‹ che khuáº¥t.

    Sau khi cÃ³ Ä‘Æ°á»£c mask, nÃ³ sáº½ Ä‘Æ°á»£c apply vÃ o X1 Ä‘á»ƒ che khuáº¥t cÃ¡c Ä‘iá»ƒm bá»‹ bá»ng vÃ  káº¥t feature thu Ä‘Æ°á»£c sau cÃ¹ng lÃ m Ä‘áº§u vÃ o cho há»‡ thá»‘ng nháº­n dáº¡ng.

    Cuá»‘i cÃ¹ng  Occlusion Pattern Predictor dá»± Ä‘oÃ¡n cÃ¡c máº«u occlusion nhÆ° 1 sá»± giÃ¡m sÃ¡t bá»• xung.

    MÃ´ hÃ¬nh táº¡o ra cÃ¡c Ä‘áº·c trÆ°ng á»Ÿ ba cáº¥p Ä‘á»™ khÃ¡c nhau, sau Ä‘Ã³ dá»± Ä‘oÃ¡n vá»‹ trÃ­ cÃ¡c vÃ¹ng bá»‹ che khuáº¥t Ä‘á»ƒ loáº¡i bá» pháº§n Ä‘áº·c trÆ°ng bá»‹ nhiá»…u, giá»¯ láº¡i pháº§n Ä‘áº·c trÆ°ng sáº¡ch cho nháº­n diá»‡n. Loss tá»•ng thá»ƒ lÃ  sá»± káº¿t há»£p cá»§a loss nháº­n diá»‡n khuÃ´n máº·t (CosFace loss) vÃ  loss dá»± Ä‘oÃ¡n máº«u che khuáº¥t (MSE hoáº·c Cross-entropy loss).

2. MÃ´ hÃ¬nh MSML (Multi-Scale Mask Learning):
    MÃ´ hÃ¬nh MSML sá»­ dá»¥ng há»c Ä‘a phÃ¢n Ä‘oáº¡n Ä‘á»ƒ xá»­ lÃ½ cÃ¡c Ä‘áº·c Ä‘iá»ƒm che khuáº¥t khÃ¡c nhau.
    
    Cáº¥u trÃºc gá»“m nhÃ¡nh nháº­n diá»‡n khuÃ´n máº·t (Face Recognition Branch - FRB), nhÃ¡nh phÃ¢n Ä‘oáº¡n che khuáº¥t (Occlusion Segmentation Branch - OSB), vÃ  cÃ¡c phÃ©p mask Ä‘áº·c trÆ°ng.
    
    Loss tá»•ng thá»ƒ gá»“m loss phÃ¢n Ä‘oáº¡n vÃ  loss nháº­n diá»‡n. 

3. PhÆ°Æ¡ng phÃ¡p vá»›i MÃ´ hÃ¬nh CR (Channel Refinement):
    PhÃ¢n chia cÃ¡c Ä‘áº·c trÆ°ng Ä‘á»ƒ dá»± Ä‘oÃ¡n che khuáº¥t vÃ  nhÃºng danh tÃ­nh (identity embedding).

    CR Network chuyá»ƒn máº·t náº¡ 2D thÃ nh 3D Ä‘á»ƒ phÃ¹ há»£p hÆ¡n vá»›i cÃ¡c báº£n Ä‘á»“ Ä‘áº·c trÆ°ng.
    
    Loss gá»“m CosFace loss Ä‘á»ƒ tá»‘i Æ°u nháº­n diá»‡n vÃ  cÃ¡c loss khÃ¡c cho dá»± Ä‘oÃ¡n máº·t náº¡.

4. Consistent Sub-decision Network: 
    Sá»­ dá»¥ng cÃ¡c quyáº¿t Ä‘á»‹nh phá»¥ (sub-decisions) dá»±a trÃªn cÃ¡c vÃ¹ng khuÃ´n máº·t khÃ¡c nhau, Ã¡p dá»¥ng KL divergence Ä‘á»ƒ hÆ°á»›ng máº¡ng táº­p trung vÃ o cÃ¡c pháº§n khuÃ´n máº·t khÃ´ng bá»‹ che khuáº¥t.

    Ãp dá»¥ng kiáº¿n thá»©c truyá»n Ä‘áº¡t (knowledge distillation) Ä‘á»ƒ Ä‘Æ°a Ä‘áº·c trÆ°ng khuÃ´n máº·t bá»‹ che hÆ°á»›ng tá»›i Ä‘áº·c trÆ°ng khuÃ´n máº·t bÃ¬nh thÆ°á»ng, giáº£m thiá»ƒu máº¥t mÃ¡t thÃ´ng tin.

5. PhÆ°Æ¡ng phÃ¡p chiáº¿n tháº¯ng cá»§a thá»­ thÃ¡ch ICCV 2021-MFR:
    
    Sá»­ dá»¥ng ká»¹ thuáº­t Ã¡nh xáº¡ máº·t náº¡ vÃ o káº¿t cáº¥u khuÃ´n máº·t vÃ  sinh áº£nh khuÃ´n máº·t bá»‹ che.

    XÃ¢y dá»±ng khung lÃ m sáº¡ch dá»¯ liá»‡u dá»±a trÃªn há»c tá»± Ä‘á»™ng, sá»­ dá»¥ng DBSCAN Ä‘á»ƒ lÃ m sáº¡ch dá»¯ liá»‡u nháº­n diá»‡n.

    Äá» xuáº¥t Balanced Curricular Loss Ä‘á»ƒ Ä‘iá»u chá»‰nh táº§m quan trá»ng cá»§a cÃ¡c máº«u dá»… vÃ  khÃ³ trong cÃ¡c giai Ä‘oáº¡n huáº¥n luyá»‡n khÃ¡c nhau.

## 4.8. Privacy-Preserving FR (báº£o vá»‡ quyá»n riÃªng tÆ°)
Bá»