# Nguồn
https://arxiv.org/pdf/1905.11946

# Abstract

Đề xuất 1 giải pháp scale mô hình có hệ thống 3 chiều depth, width, revolution để mang lại hiệu suất vượt trội gọi là compound coefficient.
Chứng minh bằng cách mở rộng MobileNets và Resnet.

# 1. Introduce

- Trước đây mấy ô mở rộng 1 hoặc 2 trong 3 chiều depth, width, revolution để mang lại hiệu quả mô hình tốt hơn. Nhưng làm thủ công dẫn đến sự tẻ nhạt và thường không mang lại kết quả tối ưu.

- Tác giả nghiên cứu nguyên tắc nào để mở rộng ConvNets có thể đạt được độ chính xác và hiệu quả tốt hơn không. Kết quả thực nghiệm cho thấy **phải cân bằng tất cả các chiều của chiều rộng/chiều sâu/độ phân giải mạng và đáng ngạc nhiên là sự cân bằng như vậy có thể đạt được chỉ bằng cách mở rộng từng chiều với tỷ lệ không đổi**. Dựa trên quan sát này đề xuất giải pháp mở rộng đồng đều cả 3 chiều với 1 tỷ lệ cố định.

- Ví dụ: nếu muốn sử dụng nhiều hơn 2^N tài nguyên tính toán, chỉ cần tăng depth lên (alpha)^N, width lên (beta)^N và image size lên (gamma)^N. Trong đó 3 hệ số alpha, beta, gamma là hyper parameter và được xác định bởi small grid search (duyệt toàn bộ).

- Họ còn đi xa hơn cả việc chứng minh tỷ lệ bằng cách cung cấp 1 mạng neutron mới gọi là EfficientNet. Với kiến trúc ít tham số hơn, FLOP hơn mà hiệu suất cho ra vượt trội so với các state-of-the-art khác.

# 2. Related work

1. Accuracy ConvNet: Nói về sự ra đời của AlexNet và hiệu suất của các mô hình sota đạt được cao nhưng quá nhiều tham số và chỉ có thể train song song.
2. Efficient ConvNet: Các mạng Deep ConvNet thường có nhiều tham số quá mức. 1 số công trình ra đời đánh đổi accuracy lây effiecient để có thể triển khai trên các thiết bị nhỏ gọn hơn như di động. Papper này đi theo hướng này, tác giả sử dụng giải pháp scaling.
3. Model Scaling: Resnet mở rộng theo chiều sâu, MobileNet mở rộng theo chiều rộng. Người ta cũng nhận ra rằng kích thước hình ảnh đầu vào lớn hơn sẽ giúp tăng độ chính xác với chi phí chung của nhiều FLOPS hơn.

# 3. Compound Model Scaling

## 3.1. Problem Formulation

![](images/6.%20Problem%20Formular.png)

Yi = Fi(Xi) với F(i) là operator, Yi là tensor output và Xi là input tensor = (Hi,Wi,Ci)

Kiến trúc của 1 ConvNet N như công thức trên. 

Các ConvNet như Resnet thường có xu hướng lặp lại 1 block nào đó và chia ra làm các stage khác nhau như Resnet. Do đó trong công thức trên Fi là công thức của 1 layer/block bị lặp đi lặp lại và Li là số lần lặp các block Fi trong stage.

![](images/6.%20Target%20Fomular.png)

Mục tiêu của bài báo này là Maximum accuracy của Conv N sao cho không vượt quá flops và memory. Bằng cách xác định các thông số d,w,r của compound scaling.

## 3.2. Scaling Dimensions

Phần này nói về những thực nhiệm mở rộng 3 chiều để accuracy có thể tốt hơn. Kết quả thu được đều là khi mở rộng đến 1 giới hạn nhất định thì accuracy hầu như không tăng nữa.

![](images/6.%20Compound%20Scale.png)

**Kết luận 1: Scale bất cứ chiều nào đều sẽ cải thiện accuracy nhưng độ tăng sẽ giảm dần khi mô hình càng lớn.**

## 3.3. Compound Scaling

Họ nhận thấy rằng việc scaling các dimention không hoàn toàn độc lập mà phụ thuộc vào nhau. Qua thực nhiệm sau, họ giữ nguyên 2 chiều d và r và chỉ mở rộng w. Kết quả thu được là mở rộng với d = 2 và r = 2 cho hiệu suất tốt hơn nhiều trong cùng 1 FLOPS.

![](images/6.%20Flops.png)

**Kết luận 2: Để đạt được accuracy tốt nhất, điều quan trọng là phải cân bằng tất cả các chiều d,w,r trong quá trình mở rộng ConvNet.**

**Compound scaling method**

![](images/6.%20Compound%20Scale.png)

# 4. EfficientNet Architecture

- Đi xa hơn bằng cách phát triển 1 mạng mới gọi là EfficientNet. Lấy cảm hứng từ (Tan et al., 2019), họ phát triển 1 baseline multi task để tối ưu hóa cả 2 thông số accuracy và FLOPS. Cụ thể họ sử dụng cùng search space và mục tiêu tối ưu hóa là ![](images/6.%20ACC%20FLOPS.png), trong đó
    - ACC(m) and FLOPS(m) là accuracy và flop của model m.
    - T là target FLOPS mà người dùng kiểm soát
    - w = -0.07: hyperparameter để kiểm soát trade-off (sự cân bằng) giữa accuracy và FLOPS.
- Khác (Tan et al., 2019), họ forcus tối ưu hóa FLOPS thay vì latence. Kiến trúc của nó tương tự MnasNet, nhưng Efficeient-B0 to hơn 1 chút do mục tiêu tối ưu FLOPS. Thành phần chính là khối mobile inverted bottleneck MBConv (Sandler et al., 2018; Tan et al., 2019) được tối ưu hóa bằng squeeze-and-excitation (Hu et al., 2018).
    ![](images/6.%20Efficientnet-B0.png)
- Bắt đầu từ mạng cơ sở EfficientNet-B0, chúng tôi áp dụng phương pháp mở rộng kết hợp của mình để mở rộng nó qua hai bước:
    - BƯỚC 1: Trước tiên, chúng tôi cố định ϕ=1, giả định tài nguyên tăng gấp đôi, và thực hiện tìm kiếm lưới nhỏ cho α,β,γ dựa trên Phương trình 2 và 3. Cụ thể, chúng tôi tìm thấy các giá trị tối ưu cho EfficientNet-B0 là α=1.2, β=1.1, γ=1.15, với ràng buộc α⋅β^2⋅γ^2≈2
    - BƯỚC 2: Sau đó, chúng tôi cố định α,β,γ làm hằng số và mở rộng mạng cơ sở với các giá trị ϕ khác nhau bằng Phương trình 3 để thu được EfficientNet-B1 đến B7 (Chi tiết trong Bảng 2)

- Đáng chú ý, có thể đạt được hiệu suất tốt hơn bằng cách tìm kiếm trực tiếp α,β,γ trên mô hình lớn, nhưng chi phí tìm kiếm trở nên quá đắt đỏ với các mô hình lớn hơn. Phương pháp của chúng tôi giải quyết vấn đề này bằng cách chỉ thực hiện tìm kiếm một lần trên mạng cơ sở nhỏ (bước 1), sau đó sử dụng cùng các hệ số mở rộng cho tất cả các mô hình khác (bước 2).

# 5. Experiments

- Các mô hình Efficient net từ B0->B7 được huấn luyện với 
    - optimizer RMSProp (decay 0.9 and momentum 0.9), 
    - batch norm momentum 0.99
    - weight decay 1e-5
    - initial learning rate 0.256 giảm learning rate 0.97 mỗi 2.4 epochs
    - Activation SiLU (Ramachandran et al., 2018; Elfwing et al., 2018; Hendrycks & Gimpel, 2016)
    - AutoAugment (Cubuk et al.,2019): Tăng cường dữ liệu tự động.
    - Stochastic Depth:  Sử dụng với xác suất sống sót là 0.8
    - Regularization: Dropout tăng tuyến tính từ 0.2 cho EfficientNet-B0 đến 0.5 cho EfficientNet-B7.
    - Early stopping.